{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy0rB8bgP3eONA94/Tn1xJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install\n",
        "---"
      ],
      "metadata": {
        "id": "4MfCc_mue7pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --quiet langchain\n",
        "#!pip install --quiet -U langchain-cohere\n",
        "#!pip install --quiet langchain-community\n",
        "#!pip install --quiet langchain-chroma\n",
        "#!pip install unstructured\n",
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-wgqk7DhU7BK",
        "outputId": "98057759-3076-4ef3-e5f9-8207dd589bd4"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA & RAG CHAIN\n",
        "---"
      ],
      "metadata": {
        "id": "SIAIKPNc89t-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# keys\n",
        "from google.colab import userdata\n",
        "COHERE_API = userdata.get('COHERE_API')"
      ],
      "metadata": {
        "id": "gUaSZ8v_X58c"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ChatCOHERE LLM\n",
        "from langchain_cohere import ChatCohere\n",
        "\n",
        "llm = ChatCohere(model=\"command-r-plus\", cohere_api_key=COHERE_API)\n",
        "llm.invoke(\"hi\")"
      ],
      "metadata": {
        "id": "9UsS1fc_Ukqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53d3067e-4738-4432-b1da-2b080cc95576"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I help you today?', additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'f6950442-204f-4f69-8d03-a311396ceb1f', 'token_count': {'input_tokens': 67, 'output_tokens': 9}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'f6950442-204f-4f69-8d03-a311396ceb1f', 'token_count': {'input_tokens': 67, 'output_tokens': 9}}, id='run-d950be36-8b7a-4b48-815c-1a8872a3d679-0')"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WEBPAGE DATA LOAD\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "#URL = \"https://docs.cohere.com/docs/retrieval-augmented-generation-rag/\"\n",
        "#URL = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
        "URL = \"https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_groq/\"\n",
        "\n",
        "# Load blog post\n",
        "loader = WebBaseLoader(URL)\n",
        "data = loader.load()\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFSWedh7gBeM",
        "outputId": "6744613d-11f8-4a3d-855b-3353834773dc"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLlama3 Cookbook with Groq - LlamaIndex\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LlamaIndex\\n          \\n\\n\\n\\n            \\n              Llama3 Cookbook with Groq\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n    \\n  \\n  Home\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Learn\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Use Cases\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Component Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Advanced Topics\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  API Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Open-Source Community\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  LlamaCloud\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LlamaIndex\\n  \\n\\n\\n\\n\\n\\n\\n    Home\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Home\\n          \\n\\n\\n\\n\\n    High-Level Concepts (RAG)\\n  \\n\\n\\n\\n\\n\\n    Installation and Setup\\n  \\n\\n\\n\\n\\n\\n    How to read these docs\\n  \\n\\n\\n\\n\\n\\n\\n    Starter Examples\\n  \\n\\n\\n\\n\\n\\n            Starter Examples\\n          \\n\\n\\n\\n\\n    Starter Tutorial (OpenAI)\\n  \\n\\n\\n\\n\\n\\n    Starter Tutorial (Local Models)\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Discover LlamaIndex Video Series\\n  \\n\\n\\n\\n\\n\\n    Frequently Asked Questions (FAQ)\\n  \\n\\n\\n\\n\\n\\n\\n\\n    Starter Tools\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Starter Tools\\n          \\n\\n\\n\\n\\n    RAG CLI\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Learn\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Learn\\n          \\n\\n\\n\\n\\n    Using LLMs\\n  \\n\\n\\n\\n\\n\\n\\n    Loading & Ingestion\\n  \\n\\n\\n\\n\\n\\n            Loading & Ingestion\\n          \\n\\n\\n\\n\\n    Loading Data (Ingestion)\\n  \\n\\n\\n\\n\\n\\n    LlamaHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Indexing & Embedding\\n  \\n\\n\\n\\n\\n\\n    Storing\\n  \\n\\n\\n\\n\\n\\n    Querying\\n  \\n\\n\\n\\n\\n\\n    Tracing and Debugging\\n  \\n\\n\\n\\n\\n\\n\\n    Evaluating\\n  \\n\\n\\n\\n\\n\\n            Evaluating\\n          \\n\\n\\n\\n\\n    Evaluating\\n  \\n\\n\\n\\n\\n\\n\\n\\n    Cost Analysis\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Cost Analysis\\n          \\n\\n\\n\\n\\n    Usage Pattern\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Putting it all Together\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Putting it all Together\\n          \\n\\n\\n\\n\\n    Agents\\n  \\n\\n\\n\\n\\n\\n    Full-Stack Web Application\\n  \\n\\n\\n\\n\\n\\n    Knowledge Graphs\\n  \\n\\n\\n\\n\\n\\n    Q&A patterns\\n  \\n\\n\\n\\n\\n\\n    Structured Data\\n  \\n\\n\\n\\n\\n\\n\\n    apps\\n  \\n\\n\\n\\n\\n\\n            apps\\n          \\n\\n\\n\\n\\n    A Guide to Building a Full-Stack Web App with LLamaIndex\\n  \\n\\n\\n\\n\\n\\n    A Guide to Building a Full-Stack LlamaIndex Web App with Delphic\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    chatbots\\n  \\n\\n\\n\\n\\n\\n            chatbots\\n          \\n\\n\\n\\n\\n    How to Build a Chatbot\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    q_and_a\\n  \\n\\n\\n\\n\\n\\n            q_and_a\\n          \\n\\n\\n\\n\\n    A Guide to Extracting Terms and Definitions\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Use Cases\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Use Cases\\n          \\n\\n\\n\\n\\n    Prompting\\n  \\n\\n\\n\\n\\n\\n    Question-Answering (RAG)\\n  \\n\\n\\n\\n\\n\\n    Chatbots\\n  \\n\\n\\n\\n\\n\\n    Structured Data Extraction\\n  \\n\\n\\n\\n\\n\\n    Agents\\n  \\n\\n\\n\\n\\n\\n    Multi-Modal Applications\\n  \\n\\n\\n\\n\\n\\n    Fine-Tuning\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Examples\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Examples\\n          \\n\\n\\n\\n\\n\\n    Agents\\n  \\n\\n\\n\\n\\n\\n            Agents\\n          \\n\\n\\n\\n\\n    \\uf8ffüí¨\\uf8ffü§ñ How to Build a Chatbot\\n  \\n\\n\\n\\n\\n\\n    Build your own OpenAI Agent\\n  \\n\\n\\n\\n\\n\\n    OpenAI agent: specifying a forced function call\\n  \\n\\n\\n\\n\\n\\n    Building a Custom Agent\\n  \\n\\n\\n\\n\\n\\n    OpenAI Assistant Advanced Retrieval Cookbook\\n  \\n\\n\\n\\n\\n\\n    Building an Agent around a Query Pipeline\\n  \\n\\n\\n\\n\\n\\n    Step-wise, Controllable Agents\\n  \\n\\n\\n\\n\\n\\n    Controllable Agents for RAG\\n  \\n\\n\\n\\n\\n\\n    Controllable Agents for RAG\\n  \\n\\n\\n\\n\\n\\n    Retrieval-Augmented OpenAI Agent\\n  \\n\\n\\n\\n\\n\\n    ReAct Agent with Query Engine (RAG) Tools\\n  \\n\\n\\n\\n\\n\\n    OpenAI Assistant Agent\\n  \\n\\n\\n\\n\\n\\n    Multi-Document Agents (V1)\\n  \\n\\n\\n\\n\\n\\n    Single-Turn Multi-Function Calling OpenAI Agents\\n  \\n\\n\\n\\n\\n\\n    ReAct Agent - A Simple Intro with Calculator Tools\\n  \\n\\n\\n\\n\\n\\n    GPT Builder Demo\\n  \\n\\n\\n\\n\\n\\n    Context-Augmented OpenAI Agent\\n  \\n\\n\\n\\n\\n\\n    Multi-Document Agents\\n  \\n\\n\\n\\n\\n\\n    OpenAI Agent with Query Engine Tools\\n  \\n\\n\\n\\n\\n\\n    OpenAI Agent + Query Engine Experimental Cookbook\\n  \\n\\n\\n\\n\\n\\n    OpenAI Agent Query Planning\\n  \\n\\n\\n\\n\\n\\n    Benchmarking OpenAI Retrieval API (through Assistant Agent)\\n  \\n\\n\\n\\n\\n\\n    Building a Multi-PDF Agent using Query Pipelines and HyDE\\n  \\n\\n\\n\\n\\n\\n    Function Calling Mistral Agent\\n  \\n\\n\\n\\n\\n\\n    OpenAI Agent with Tool Call Parser\\n  \\n\\n\\n\\n\\n\\n    Controlling Agent Reasoning Loop with Return Direct Tools\\n  \\n\\n\\n\\n\\n\\n    Function Calling Anthropic Agent\\n  \\n\\n\\n\\n\\n\\n    Chain-of-Abstraction LlamaPack\\n  \\n\\n\\n\\n\\n\\n    Language Agent Tree Search\\n  \\n\\n\\n\\n\\n\\n    LLM Compiler Agent Cookbook\\n  \\n\\n\\n\\n\\n\\n    Structured Planning Agent\\n  \\n\\n\\n\\n\\n\\n    Introspective Agents: Performing Tasks With Reflection\\n  \\n\\n\\n\\n\\n\\n    OpenAI Agent Workarounds for Lengthy Tool Descriptions\\n  \\n\\n\\n\\n\\n\\n    Vector Memory\\n  \\n\\n\\n\\n\\n\\n    Simple Composable Memory\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Callbacks\\n  \\n\\n\\n\\n\\n\\n            Callbacks\\n          \\n\\n\\n\\n\\n    HoneyHive LlamaIndex Tracer\\n  \\n\\n\\n\\n\\n\\n    PromptLayer Handler\\n  \\n\\n\\n\\n\\n\\n    Token Counting Handler\\n  \\n\\n\\n\\n\\n\\n    Llama Debug Handler\\n  \\n\\n\\n\\n\\n\\n    Observability with OpenLLMetry\\n  \\n\\n\\n\\n\\n\\n    UpTrain Callback Handler\\n  \\n\\n\\n\\n\\n\\n    Wandb Callback Handler\\n  \\n\\n\\n\\n\\n\\n    Aim Callback\\n  \\n\\n\\n\\n\\n\\n    OpenInference Callback Handler + Arize Phoenix\\n  \\n\\n\\n\\n\\n\\n    Langfuse Callback Handler\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Chat Engines\\n  \\n\\n\\n\\n\\n\\n            Chat Engines\\n          \\n\\n\\n\\n\\n    Chat Engine with a Personality ‚ú®\\n  \\n\\n\\n\\n\\n\\n    Chat Engine - OpenAI Agent Mode\\n  \\n\\n\\n\\n\\n\\n    Chat Engine - Context Mode\\n  \\n\\n\\n\\n\\n\\n    Chat Engine - Best Mode\\n  \\n\\n\\n\\n\\n\\n    Chat Engine - ReAct Agent Mode\\n  \\n\\n\\n\\n\\n\\n    Chat Engine - Simple Mode REPL\\n  \\n\\n\\n\\n\\n\\n    Chat Engine - Condense Plus Context Mode\\n  \\n\\n\\n\\n\\n\\n    Chat Engine - Condense Question Mode\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Cookbooks\\n  \\n\\n\\n\\n\\n\\n            Cookbooks\\n          \\n\\n\\n\\n\\n    Cohere init8 and binary Embeddings Retrieval Evaluation\\n  \\n\\n\\n\\n\\n\\n    mixedbread Rerank Cookbook\\n  \\n\\n\\n\\n\\n\\n    MistralAI Cookbook\\n  \\n\\n\\n\\n\\n\\n    Anthropic Haiku Cookbook\\n  \\n\\n\\n\\n\\n\\n    Llama3 Cookbook\\n  \\n\\n\\n\\n\\n\\n\\n    Llama3 Cookbook with Groq\\n  \\n\\n\\n\\n\\n    Llama3 Cookbook with Groq\\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Installation and Setup\\n    \\n\\n\\n\\n\\n\\n\\n      Setup LLM using Groq\\n    \\n\\n\\n\\n\\n\\n      Setup Embedding Model\\n    \\n\\n\\n\\n\\n\\n      Define Global Settings Configuration\\n    \\n\\n\\n\\n\\n\\n      Download Data\\n    \\n\\n\\n\\n\\n\\n      Load Data\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      1. Basic Completion and Chat\\n    \\n\\n\\n\\n\\n\\n\\n      Call complete with a prompt\\n    \\n\\n\\n\\n\\n\\n      Call chat with a list of messages\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      2. Basic RAG (Vector Search, Summarization)\\n    \\n\\n\\n\\n\\n\\n\\n      Basic RAG (Vector Search)\\n    \\n\\n\\n\\n\\n\\n      Basic RAG (Summarization)\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      3. Advanced RAG (Routing)\\n    \\n\\n\\n\\n\\n\\n\\n      Build a Router that can choose whether to do vector search or summarization\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      4. Text-to-SQL\\n    \\n\\n\\n\\n\\n\\n      5. Structured Data Extraction\\n    \\n\\n\\n\\n\\n\\n      6. Adding Chat History to RAG (Chat Engine)\\n    \\n\\n\\n\\n\\n\\n      7. Agents\\n    \\n\\n\\n\\n\\n\\n\\n      Agents And Tools\\n    \\n\\n\\n\\n\\n\\n      Define Tools\\n    \\n\\n\\n\\n\\n\\n      ReAct Agent\\n    \\n\\n\\n\\n\\n\\n      Querying\\n    \\n\\n\\n\\n\\n\\n      ReAct Agent With RAG QueryEngine Tools\\n    \\n\\n\\n\\n\\n\\n      Create ReAct Agent using RAG QueryEngine Tools\\n    \\n\\n\\n\\n\\n\\n      Querying\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Llama3 Cookbook with Ollama and Replicate\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Customization\\n  \\n\\n\\n\\n\\n\\n            Customization\\n          \\n\\n\\n\\n\\n    Streaming for Chat Engine - Condense Question Mode\\n  \\n\\n\\n\\n\\n\\n    Streaming\\n  \\n\\n\\n\\n\\n\\n    Completion Prompts Customization\\n  \\n\\n\\n\\n\\n\\n    Chat Prompts Customization\\n  \\n\\n\\n\\n\\n\\n    ChatGPT\\n  \\n\\n\\n\\n\\n\\n    HuggingFace LLM - StableLM\\n  \\n\\n\\n\\n\\n\\n    HuggingFace LLM - Camel-5b\\n  \\n\\n\\n\\n\\n\\n    Azure OpenAI\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Data Connectors\\n  \\n\\n\\n\\n\\n\\n            Data Connectors\\n          \\n\\n\\n\\n\\n    Parallel Processing SimpleDirectoryReader\\n  \\n\\n\\n\\n\\n\\n    DeepLake Reader\\n  \\n\\n\\n\\n\\n\\n    Psychic Reader\\n  \\n\\n\\n\\n\\n\\n    Qdrant Reader\\n  \\n\\n\\n\\n\\n\\n    HTML Tag Reader\\n  \\n\\n\\n\\n\\n\\n    Discord Reader\\n  \\n\\n\\n\\n\\n\\n    MongoDB Reader\\n  \\n\\n\\n\\n\\n\\n    Chroma Reader\\n  \\n\\n\\n\\n\\n\\n    MyScale Reader\\n  \\n\\n\\n\\n\\n\\n    Faiss Reader\\n  \\n\\n\\n\\n\\n\\n    Obsidian Reader\\n  \\n\\n\\n\\n\\n\\n    Slack Reader\\n  \\n\\n\\n\\n\\n\\n    Web Page Reader\\n  \\n\\n\\n\\n\\n\\n    Pinecone Reader\\n  \\n\\n\\n\\n\\n\\n    Mbox Reader\\n  \\n\\n\\n\\n\\n\\n    MilvusReader\\n  \\n\\n\\n\\n\\n\\n    Notion Reader\\n  \\n\\n\\n\\n\\n\\n    DashVector Reader\\n  \\n\\n\\n\\n\\n\\n    Pathway Reader\\n  \\n\\n\\n\\n\\n\\n    Deplot Reader Demo\\n  \\n\\n\\n\\n\\n\\n    Github Repo Reader\\n  \\n\\n\\n\\n\\n\\n    Simple Directory Reader\\n  \\n\\n\\n\\n\\n\\n    Google Docs Reader\\n  \\n\\n\\n\\n\\n\\n    Database Reader\\n  \\n\\n\\n\\n\\n\\n    Twitter Reader\\n  \\n\\n\\n\\n\\n\\n    Weaviate Reader\\n  \\n\\n\\n\\n\\n\\n    Make Reader\\n  \\n\\n\\n\\n\\n\\n    Google Sheets Reader\\n  \\n\\n\\n\\n\\n\\n    Simple Directory Reader over a Remote FileSystem\\n  \\n\\n\\n\\n\\n\\n    Google Drive Reader\\n  \\n\\n\\n\\n\\n\\n    None\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Discover LlamaIndex\\n  \\n\\n\\n\\n\\n\\n            Discover LlamaIndex\\n          \\n\\n\\n\\n\\n    Discord Thread Management\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Docstores\\n  \\n\\n\\n\\n\\n\\n            Docstores\\n          \\n\\n\\n\\n\\n    Dynamo DB Docstore Demo\\n  \\n\\n\\n\\n\\n\\n    Redis Docstore+Index Store Demo\\n  \\n\\n\\n\\n\\n\\n    MongoDB Demo\\n  \\n\\n\\n\\n\\n\\n    Firestore Demo\\n  \\n\\n\\n\\n\\n\\n    Docstore Demo\\n  \\n\\n\\n\\n\\n\\n    Demo: Azure Table Storage as a Docstore\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Embeddings\\n  \\n\\n\\n\\n\\n\\n            Embeddings\\n          \\n\\n\\n\\n\\n    Qdrant FastEmbed Embeddings\\n  \\n\\n\\n\\n\\n\\n    Text Embedding Inference\\n  \\n\\n\\n\\n\\n\\n    Embeddings with Clarifai\\n  \\n\\n\\n\\n\\n\\n    Bedrock Embeddings\\n  \\n\\n\\n\\n\\n\\n    Voyage Embeddings\\n  \\n\\n\\n\\n\\n\\n    Ollama Embeddings\\n  \\n\\n\\n\\n\\n\\n    Gradient Embeddings\\n  \\n\\n\\n\\n\\n\\n    Custom Embeddings\\n  \\n\\n\\n\\n\\n\\n    Google Gemini Embeddings\\n  \\n\\n\\n\\n\\n\\n    Local Embeddings with HuggingFace\\n  \\n\\n\\n\\n\\n\\n    Anyscale Embeddings\\n  \\n\\n\\n\\n\\n\\n    Optimized Embedding Model using Optimum-Intel\\n  \\n\\n\\n\\n\\n\\n    Jina Embeddings\\n  \\n\\n\\n\\n\\n\\n    Fireworks Embeddings\\n  \\n\\n\\n\\n\\n\\n    Nomic Embedding\\n  \\n\\n\\n\\n\\n\\n    MistralAI Embeddings\\n  \\n\\n\\n\\n\\n\\n    Dashscope embeddings\\n  \\n\\n\\n\\n\\n\\n    Jina 8K Context Window Embeddings\\n  \\n\\n\\n\\n\\n\\n    LLMRails Embeddings\\n  \\n\\n\\n\\n\\n\\n    Google PaLM Embeddings\\n  \\n\\n\\n\\n\\n\\n    Interacting with Embeddings deployed in Amazon SageMaker Endpoint with LlamaIndex\\n  \\n\\n\\n\\n\\n\\n    LangChain Embeddings\\n  \\n\\n\\n\\n\\n\\n    Elasticsearch Embeddings\\n  \\n\\n\\n\\n\\n\\n    OpenAI Embeddings\\n  \\n\\n\\n\\n\\n\\n    CohereAI Embeddings\\n  \\n\\n\\n\\n\\n\\n    Together AI Embeddings\\n  \\n\\n\\n\\n\\n\\n    Llamafile Embeddings\\n  \\n\\n\\n\\n\\n\\n    PremAI Embeddings\\n  \\n\\n\\n\\n\\n\\n    Aleph Alpha Embeddings\\n  \\n\\n\\n\\n\\n\\n    Optimized BGE Embedding Model using Intel¬Æ Extension for Transformers\\n  \\n\\n\\n\\n\\n\\n    Cloudflare Workers AI Embeddings\\n  \\n\\n\\n\\n\\n\\n    Local Embeddings with OpenVINO\\n  \\n\\n\\n\\n\\n\\n    Local Embeddings with IPEX-LLM on Intel CPU\\n  \\n\\n\\n\\n\\n\\n    OctoAI Embeddings\\n  \\n\\n\\n\\n\\n\\n    Local Embeddings with IPEX-LLM on Intel GPU\\n  \\n\\n\\n\\n\\n\\n    NVIDIA NIMs\\n  \\n\\n\\n\\n\\n\\n    None\\n  \\n\\n\\n\\n\\n\\n    Upstage Embeddings\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Evaluation\\n  \\n\\n\\n\\n\\n\\n            Evaluation\\n          \\n\\n\\n\\n\\n    Tonic Validate Evaluators\\n  \\n\\n\\n\\n\\n\\n    Embedding Similarity Evaluator\\n  \\n\\n\\n\\n\\n\\n    BatchEvalRunner - Running Multiple Evaluations\\n  \\n\\n\\n\\n\\n\\n    Benchmarking LLM Evaluators On The MT-Bench Human Judgement LabelledPairwiseEvaluatorDataset\\n  \\n\\n\\n\\n\\n\\n    Benchmarking LLM Evaluators On A Mini MT-Bench (Single Grading) LabelledEvaluatorDataset\\n  \\n\\n\\n\\n\\n\\n    Answer Relevancy and Context Relevancy Evaluations\\n  \\n\\n\\n\\n\\n\\n    Evaluation using Prometheus model\\n  \\n\\n\\n\\n\\n\\n    Faithfulness Evaluator\\n  \\n\\n\\n\\n\\n\\n    HotpotQADistractor Demo\\n  \\n\\n\\n\\n\\n\\n    Self Correcting Query Engines - Evaluation & Retry\\n  \\n\\n\\n\\n\\n\\n    Correctness Evaluator\\n  \\n\\n\\n\\n\\n\\n    How to use UpTrain with LlamaIndex\\n  \\n\\n\\n\\n\\n\\n    QuestionGeneration\\n  \\n\\n\\n\\n\\n\\n    Retrieval Evaluation\\n  \\n\\n\\n\\n\\n\\n    Evaluating Multi-Modal RAG\\n  \\n\\n\\n\\n\\n\\n    BEIR Out of Domain Benchmark\\n  \\n\\n\\n\\n\\n\\n    Relevancy Evaluator\\n  \\n\\n\\n\\n\\n\\n    \\uf8ffüöÄ RAG/LLM Evaluators - DeepEval\\n  \\n\\n\\n\\n\\n\\n    Guideline Evaluator\\n  \\n\\n\\n\\n\\n\\n    Pairwise Evaluator\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Finetuning\\n  \\n\\n\\n\\n\\n\\n            Finetuning\\n          \\n\\n\\n\\n\\n    Fine Tuning Llama2 for Better Structured Outputs With Gradient and LlamaIndex\\n  \\n\\n\\n\\n\\n\\n    Fine Tuning Nous-Hermes-2 With Gradient and LlamaIndex\\n  \\n\\n\\n\\n\\n\\n    Fine Tuning for Text-to-SQL With Gradient and LlamaIndex\\n  \\n\\n\\n\\n\\n\\n    Finetune Embeddings\\n  \\n\\n\\n\\n\\n\\n    Finetuning an Adapter on Top of any Black-Box Embedding Model\\n  \\n\\n\\n\\n\\n\\n    Fine Tuning with Function Calling\\n  \\n\\n\\n\\n\\n\\n    Custom Cohere Reranker\\n  \\n\\n\\n\\n\\n\\n    Fine Tuning GPT-3.5-Turbo\\n  \\n\\n\\n\\n\\n\\n    How to Finetune a cross-encoder using LLamaIndex\\n  \\n\\n\\n\\n\\n\\n    Fine-tuning a gpt-3.5 ReAct Agent on Better Chain of Thought\\n  \\n\\n\\n\\n\\n\\n    Knowledge Distillation For Fine-Tuning A GPT-3.5 Judge (Pairwise)\\n  \\n\\n\\n\\n\\n\\n    Knowledge Distillation For Fine-Tuning A GPT-3.5 Judge (Correctness)\\n  \\n\\n\\n\\n\\n\\n    Router Fine-tuning\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Ingestion\\n  \\n\\n\\n\\n\\n\\n            Ingestion\\n          \\n\\n\\n\\n\\n    Async Ingestion Pipeline + Metadata Extraction\\n  \\n\\n\\n\\n\\n\\n    Parallelizing Ingestion Pipeline\\n  \\n\\n\\n\\n\\n\\n    Ingestion Pipeline + Document Management\\n  \\n\\n\\n\\n\\n\\n    Building a Live RAG Pipeline over Google Drive Files\\n  \\n\\n\\n\\n\\n\\n    Advanced Ingestion Pipeline\\n  \\n\\n\\n\\n\\n\\n    Redis Ingestion Pipeline\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Llama Datasets\\n  \\n\\n\\n\\n\\n\\n            Llama Datasets\\n          \\n\\n\\n\\n\\n    Contributing a LlamaDataset To LlamaHub\\n  \\n\\n\\n\\n\\n\\n    Benchmarking RAG Pipelines With A LabelledRagDatatset\\n  \\n\\n\\n\\n\\n\\n    Downloading a LlamaDataset from LlamaHub\\n  \\n\\n\\n\\n\\n\\n    LlamaDataset Submission Template Notebook\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Llama Hub\\n  \\n\\n\\n\\n\\n\\n            Llama Hub\\n          \\n\\n\\n\\n\\n    Ollama Llama Pack Example\\n  \\n\\n\\n\\n\\n\\n    Llama Packs Example\\n  \\n\\n\\n\\n\\n\\n    LlamaHub Demostration\\n  \\n\\n\\n\\n\\n\\n    Llama Pack - Resume Screener \\uf8ffüìÑ\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LLMs\\n  \\n\\n\\n\\n\\n\\n            LLMs\\n          \\n\\n\\n\\n\\n    RunGPT\\n  \\n\\n\\n\\n\\n\\n    WatsonX\\n  \\n\\n\\n\\n\\n\\n    OpenLLM\\n  \\n\\n\\n\\n\\n\\n    OpenAI JSON Mode vs. Function Calling for Data Extraction\\n  \\n\\n\\n\\n\\n\\n    MyMagic AI LLM\\n  \\n\\n\\n\\n\\n\\n    Portkey\\n  \\n\\n\\n\\n\\n\\n    EverlyAI\\n  \\n\\n\\n\\n\\n\\n    PaLM\\n  \\n\\n\\n\\n\\n\\n    Cohere\\n  \\n\\n\\n\\n\\n\\n    Vertex AI\\n  \\n\\n\\n\\n\\n\\n    Predibase\\n  \\n\\n\\n\\n\\n\\n    Llama API\\n  \\n\\n\\n\\n\\n\\n    Clarifai LLM\\n  \\n\\n\\n\\n\\n\\n    Bedrock\\n  \\n\\n\\n\\n\\n\\n    Replicate - Llama 2 13B\\n  \\n\\n\\n\\n\\n\\n    Gradient Model Adapter\\n  \\n\\n\\n\\n\\n\\n    Maritalk\\n  \\n\\n\\n\\n\\n\\n    Nvidia TensorRT-LLM\\n  \\n\\n\\n\\n\\n\\n    Xorbits Inference\\n  \\n\\n\\n\\n\\n\\n    Azure OpenAI\\n  \\n\\n\\n\\n\\n\\n    Gemini\\n  \\n\\n\\n\\n\\n\\n    Hugging Face LLMs\\n  \\n\\n\\n\\n\\n\\n    Anyscale\\n  \\n\\n\\n\\n\\n\\n    Replicate - Vicuna 13B\\n  \\n\\n\\n\\n\\n\\n    OpenRouter\\n  \\n\\n\\n\\n\\n\\n    Fireworks\\n  \\n\\n\\n\\n\\n\\n    \\uf8ffü¶ô x \\uf8ffü¶ô Rap Battle\\n  \\n\\n\\n\\n\\n\\n    vLLM\\n  \\n\\n\\n\\n\\n\\n    DashScope LLMS\\n  \\n\\n\\n\\n\\n\\n    LocalAI\\n  \\n\\n\\n\\n\\n\\n    LLM Predictor\\n  \\n\\n\\n\\n\\n\\n    MistralAI\\n  \\n\\n\\n\\n\\n\\n    Monster API <> LLamaIndex\\n  \\n\\n\\n\\n\\n\\n    AI21\\n  \\n\\n\\n\\n\\n\\n    LlamaCPP\\n  \\n\\n\\n\\n\\n\\n    Nvidia Triton\\n  \\n\\n\\n\\n\\n\\n    Perplexity\\n  \\n\\n\\n\\n\\n\\n    LiteLLM\\n  \\n\\n\\n\\n\\n\\n    Ollama - Llama 3\\n  \\n\\n\\n\\n\\n\\n    Neutrino AI\\n  \\n\\n\\n\\n\\n\\n    Groq\\n  \\n\\n\\n\\n\\n\\n    Langchain\\n  \\n\\n\\n\\n\\n\\n    Interacting with LLM deployed in Amazon SageMaker Endpoint with LlamaIndex\\n  \\n\\n\\n\\n\\n\\n    OpenAI\\n  \\n\\n\\n\\n\\n\\n    Anthropic\\n  \\n\\n\\n\\n\\n\\n    Gradient Base Model\\n  \\n\\n\\n\\n\\n\\n    Ollama - Gemma\\n  \\n\\n\\n\\n\\n\\n    Konko\\n  \\n\\n\\n\\n\\n\\n    Together AI LLM\\n  \\n\\n\\n\\n\\n\\n    Fireworks Function Calling Cookbook\\n  \\n\\n\\n\\n\\n\\n    Friendli\\n  \\n\\n\\n\\n\\n\\n    ModelScope LLMS\\n  \\n\\n\\n\\n\\n\\n    llamafile\\n  \\n\\n\\n\\n\\n\\n    PremAI LlamaIndex\\n  \\n\\n\\n\\n\\n\\n    Solar LLM\\n  \\n\\n\\n\\n\\n\\n    Aleph Alpha\\n  \\n\\n\\n\\n\\n\\n    IPEX-LLM\\n  \\n\\n\\n\\n\\n\\n    DataBricks\\n  \\n\\n\\n\\n\\n\\n    OpenVINO LLMs\\n  \\n\\n\\n\\n\\n\\n    OctoAI\\n  \\n\\n\\n\\n\\n\\n    MistralRS LLM\\n  \\n\\n\\n\\n\\n\\n    Using NVIDIA\\'s LLM API Catalog Connector\\n  \\n\\n\\n\\n\\n\\n    None\\n  \\n\\n\\n\\n\\n\\n    Upstage\\n  \\n\\n\\n\\n\\n\\n    LM Studio\\n  \\n\\n\\n\\n\\n\\n    Unify\\n  \\n\\n\\n\\n\\n\\n    IPEX-LLM\\n  \\n\\n\\n\\n\\n\\n    DeepInfra\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Low Level\\n  \\n\\n\\n\\n\\n\\n            Low Level\\n          \\n\\n\\n\\n\\n    Building RAG from Scratch (Open-source only!)\\n  \\n\\n\\n\\n\\n\\n    Building an Advanced Fusion Retriever from Scratch\\n  \\n\\n\\n\\n\\n\\n    Building a Router from Scratch\\n  \\n\\n\\n\\n\\n\\n    Building Retrieval from Scratch\\n  \\n\\n\\n\\n\\n\\n    Building Evaluation from Scratch\\n  \\n\\n\\n\\n\\n\\n    Building Response Synthesis from Scratch\\n  \\n\\n\\n\\n\\n\\n    Building a (Very Simple) Vector Store from Scratch\\n  \\n\\n\\n\\n\\n\\n    Building Data Ingestion from Scratch\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Managed Indexes\\n  \\n\\n\\n\\n\\n\\n            Managed Indexes\\n          \\n\\n\\n\\n\\n    Vectara Managed Index\\n  \\n\\n\\n\\n\\n\\n    Semantic Retriever Benchmark\\n  \\n\\n\\n\\n\\n\\n    Google Generative Language Semantic Retriever\\n  \\n\\n\\n\\n\\n\\n    Managed Index with Zilliz Cloud Pipelines\\n  \\n\\n\\n\\n\\n\\n    PostgresML Managed Index\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Metadata Extractors\\n  \\n\\n\\n\\n\\n\\n            Metadata Extractors\\n          \\n\\n\\n\\n\\n    Metadata Extraction and Augmentation w/ Marvin\\n  \\n\\n\\n\\n\\n\\n    Automated Metadata Extraction for Better Retrieval + Synthesis\\n  \\n\\n\\n\\n\\n\\n    Pydantic Extractor\\n  \\n\\n\\n\\n\\n\\n    Entity Metadata Extraction\\n  \\n\\n\\n\\n\\n\\n    Extracting Metadata for Better Document Indexing and Understanding\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Multi-Modal\\n  \\n\\n\\n\\n\\n\\n            Multi-Modal\\n          \\n\\n\\n\\n\\n    LlaVa Demo with LlamaIndex\\n  \\n\\n\\n\\n\\n\\n    Retrieval-Augmented Image Captioning\\n  \\n\\n\\n\\n\\n\\n    Multi-Modal LLM using Replicate LlaVa, Fuyu 8B, MiniGPT4 models for image reasoning\\n  \\n\\n\\n\\n\\n\\n    Semi-structured Image Retrieval\\n  \\n\\n\\n\\n\\n\\n    GPT4-V Experiments with General, Specific questions and Chain Of Thought (COT) Prompting Technique.\\n  \\n\\n\\n\\n\\n\\n    Multi-Modal Retrieval using GPT text embedding and CLIP image embedding for Wikipedia Articles\\n  \\n\\n\\n\\n\\n\\n    Multi-Modal LLM using DashScope qwen-vl model for image reasoning\\n  \\n\\n\\n\\n\\n\\n    Multi-Modal LLM using OpenAI GPT-4V model for image reasoning\\n  \\n\\n\\n\\n\\n\\n    [Beta] Multi-modal ReAct Agent\\n  \\n\\n\\n\\n\\n\\n    Multi-Modal LLM using Azure OpenAI GPT-4V model for image reasoning\\n  \\n\\n\\n\\n\\n\\n    Multi-Modal LLM using Google\\'s Gemini model for image understanding and build Retrieval Augmented Generation with LlamaIndex\\n  \\n\\n\\n\\n\\n\\n    Multimodal RAG for processing videos using OpenAI GPT4V and LanceDB vectorstore\\n  \\n\\n\\n\\n\\n\\n    Multimodal Ollama Cookbook\\n  \\n\\n\\n\\n\\n\\n    Chroma Multi-Modal Demo with LlamaIndex\\n  \\n\\n\\n\\n\\n\\n    Multi-Modal GPT4V Pydantic Program\\n  \\n\\n\\n\\n\\n\\n    Advanced Multi-Modal Retrieval using GPT4V and Multi-Modal Index/Retriever\\n  \\n\\n\\n\\n\\n\\n    Image to Image Retrieval using CLIP embedding and image correlation reasoning using GPT4V\\n  \\n\\n\\n\\n\\n\\n    Multi-Modal LLM using Anthropic model for image reasoning\\n  \\n\\n\\n\\n\\n\\n    Multimodal Structured Outputs: GPT-4o vs. Other GPT-4 Variants\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Multi-Tenancy\\n  \\n\\n\\n\\n\\n\\n            Multi-Tenancy\\n          \\n\\n\\n\\n\\n    Multi-Tenancy RAG with LlamaIndex\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Node Parsers & Text Splitters\\n  \\n\\n\\n\\n\\n\\n            Node Parsers & Text Splitters\\n          \\n\\n\\n\\n\\n    Semantic Chunker\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Node Postprocessors\\n  \\n\\n\\n\\n\\n\\n            Node Postprocessors\\n          \\n\\n\\n\\n\\n    File Based Node Parsers\\n  \\n\\n\\n\\n\\n\\n    Metadata Replacement + Node Sentence Window\\n  \\n\\n\\n\\n\\n\\n    PII Masking\\n  \\n\\n\\n\\n\\n\\n    Forward/Backward Augmentation\\n  \\n\\n\\n\\n\\n\\n    RankGPT Reranker Demonstration (Van Gogh Wiki)\\n  \\n\\n\\n\\n\\n\\n    LLM Reranker Demonstration (Great Gatsby)\\n  \\n\\n\\n\\n\\n\\n    SentenceTransformerRerank\\n  \\n\\n\\n\\n\\n\\n    LLM Reranker Demonstration (2021 Lyft 10-k)\\n  \\n\\n\\n\\n\\n\\n    LongContextReorder\\n  \\n\\n\\n\\n\\n\\n    Cohere Rerank\\n  \\n\\n\\n\\n\\n\\n    Recency Filtering\\n  \\n\\n\\n\\n\\n\\n    Colbert Rerank\\n  \\n\\n\\n\\n\\n\\n    FlagEmbeddingReranker\\n  \\n\\n\\n\\n\\n\\n    Sentence Embedding Optimizer\\n  \\n\\n\\n\\n\\n\\n    Time-Weighted Rerank\\n  \\n\\n\\n\\n\\n\\n    Jina Rerank\\n  \\n\\n\\n\\n\\n\\n    RankLLM Reranker Demonstration (Van Gogh Wiki)\\n  \\n\\n\\n\\n\\n\\n    OpenVINO Rerank\\n  \\n\\n\\n\\n\\n\\n    NVIDIA NIMs\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Object Stores\\n  \\n\\n\\n\\n\\n\\n            Object Stores\\n          \\n\\n\\n\\n\\n    The ObjectIndex Class\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Output Parsers\\n  \\n\\n\\n\\n\\n\\n            Output Parsers\\n          \\n\\n\\n\\n\\n    LLM Pydantic Program\\n  \\n\\n\\n\\n\\n\\n    Function Calling Program for Structured Extraction\\n  \\n\\n\\n\\n\\n\\n    OpenAI Pydantic Program\\n  \\n\\n\\n\\n\\n\\n    DataFrame Structured Data Extraction\\n  \\n\\n\\n\\n\\n\\n    Evaporate Demo\\n  \\n\\n\\n\\n\\n\\n    OpenAI function calling for Sub-Question Query Engine\\n  \\n\\n\\n\\n\\n\\n    Guidance Pydantic Program\\n  \\n\\n\\n\\n\\n\\n    Guardrails Output Parsing\\n  \\n\\n\\n\\n\\n\\n    Langchain Output Parsing\\n  \\n\\n\\n\\n\\n\\n    LM Format Enforcer Pydantic Program\\n  \\n\\n\\n\\n\\n\\n    LM Format Enforcer Regular Expression Generation\\n  \\n\\n\\n\\n\\n\\n    Guidance for Sub-Question Query Engine\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Param Optimizer\\n  \\n\\n\\n\\n\\n\\n            Param Optimizer\\n          \\n\\n\\n\\n\\n    [WIP] Hyperparameter Optimization for RAG\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Query Pipeline\\n  \\n\\n\\n\\n\\n\\n            Query Pipeline\\n          \\n\\n\\n\\n\\n    An Introduction to LlamaIndex Query Pipelines\\n  \\n\\n\\n\\n\\n\\n    Query Pipeline over Pandas DataFrames\\n  \\n\\n\\n\\n\\n\\n    Query Pipeline for Advanced Text-to-SQL\\n  \\n\\n\\n\\n\\n\\n    Query Pipeline with Async/Parallel Execution\\n  \\n\\n\\n\\n\\n\\n    Query Pipeline with Routing\\n  \\n\\n\\n\\n\\n\\n    Query Pipeline Chat Engine\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Prompts\\n  \\n\\n\\n\\n\\n\\n            Prompts\\n          \\n\\n\\n\\n\\n    Advanced Prompt Techniques (Variable Mappings, Functions)\\n  \\n\\n\\n\\n\\n\\n    EmotionPrompt in RAG\\n  \\n\\n\\n\\n\\n\\n    Prompt Engineering for RAG\\n  \\n\\n\\n\\n\\n\\n    Accessing/Customizing Prompts within Higher-Level Modules\\n  \\n\\n\\n\\n\\n\\n    \"Optimization by Prompting\" for RAG\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Query Engines\\n  \\n\\n\\n\\n\\n\\n            Query Engines\\n          \\n\\n\\n\\n\\n    Knowledge Graph RAG Query Engine\\n  \\n\\n\\n\\n\\n\\n    JSONalyze Query Engine\\n  \\n\\n\\n\\n\\n\\n    Retriever Router Query Engine\\n  \\n\\n\\n\\n\\n\\n    [Beta] Text-to-SQL with PGVector\\n  \\n\\n\\n\\n\\n\\n    SQL Join Query Engine\\n  \\n\\n\\n\\n\\n\\n    CitationQueryEngine\\n  \\n\\n\\n\\n\\n\\n    Pandas Query Engine\\n  \\n\\n\\n\\n\\n\\n    Ensemble Query Engine Guide\\n  \\n\\n\\n\\n\\n\\n    JSON Query Engine\\n  \\n\\n\\n\\n\\n\\n    Router Query Engine\\n  \\n\\n\\n\\n\\n\\n    Query Engine with Pydantic Outputs\\n  \\n\\n\\n\\n\\n\\n    Cogniswitch query engine\\n  \\n\\n\\n\\n\\n\\n    Recursive Retriever + Query Engine Demo\\n  \\n\\n\\n\\n\\n\\n    SQL Router Query Engine\\n  \\n\\n\\n\\n\\n\\n    Joint Tabular/Semantic QA over Tesla 10K\\n  \\n\\n\\n\\n\\n\\n    Recursive Retriever + Document Agents\\n  \\n\\n\\n\\n\\n\\n    Joint QA Summary Query Engine\\n  \\n\\n\\n\\n\\n\\n    Structured Hierarchical Retrieval\\n  \\n\\n\\n\\n\\n\\n    FLARE Query Engine\\n  \\n\\n\\n\\n\\n\\n    Knowledge Graph Query Engine\\n  \\n\\n\\n\\n\\n\\n    Sub Question Query Engine\\n  \\n\\n\\n\\n\\n\\n    SQL Auto Vector Query Engine\\n  \\n\\n\\n\\n\\n\\n    Defining a Custom Query Engine\\n  \\n\\n\\n\\n\\n\\n    Retriever Query Engine with Custom Retrievers - Simple Hybrid Search\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Query Transformations\\n  \\n\\n\\n\\n\\n\\n            Query Transformations\\n          \\n\\n\\n\\n\\n    Query Transform Cookbook\\n  \\n\\n\\n\\n\\n\\n    HyDE Query Transform\\n  \\n\\n\\n\\n\\n\\n    Multi-Step Query Engine\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Response Synthesizers\\n  \\n\\n\\n\\n\\n\\n            Response Synthesizers\\n          \\n\\n\\n\\n\\n    Pydantic Tree Summarize\\n  \\n\\n\\n\\n\\n\\n    Refine\\n  \\n\\n\\n\\n\\n\\n    Refine with Structured Answer Filtering\\n  \\n\\n\\n\\n\\n\\n    Pydantic Tree Summarize\\n  \\n\\n\\n\\n\\n\\n    Stress-Testing Long Context LLMs with a Recall Task\\n  \\n\\n\\n\\n\\n\\n    Tree Summarize\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Retrievers\\n  \\n\\n\\n\\n\\n\\n            Retrievers\\n          \\n\\n\\n\\n\\n    BM25 Retriever\\n  \\n\\n\\n\\n\\n\\n    Composable Objects\\n  \\n\\n\\n\\n\\n\\n    Router Retriever\\n  \\n\\n\\n\\n\\n\\n    Recursive Retriever + Node References\\n  \\n\\n\\n\\n\\n\\n    Chunk + Document Hybrid Retrieval with Long-Context Embeddings (Together.ai)\\n  \\n\\n\\n\\n\\n\\n    Auto-Retrieval from a Vectara Index\\n  \\n\\n\\n\\n\\n\\n    Pathway Retriever\\n  \\n\\n\\n\\n\\n\\n    Comparing Methods for Structured Retrieval (Auto-Retrieval vs. Recursive Retrieval)\\n  \\n\\n\\n\\n\\n\\n    Ensemble Retrieval Guide\\n  \\n\\n\\n\\n\\n\\n    Simple Fusion Retriever\\n  \\n\\n\\n\\n\\n\\n    Auto Merging Retriever\\n  \\n\\n\\n\\n\\n\\n    Recursive Retriever + Node References + Braintrust\\n  \\n\\n\\n\\n\\n\\n    Activeloop Deep Memory\\n  \\n\\n\\n\\n\\n\\n    You.com Retriever\\n  \\n\\n\\n\\n\\n\\n    Reciprocal Rerank Fusion Retriever\\n  \\n\\n\\n\\n\\n\\n    Relative Score Fusion and Distribution-Based Score Fusion\\n  \\n\\n\\n\\n\\n\\n    VideoDB Retriever\\n  \\n\\n\\n\\n\\n\\n    Bedrock (Knowledge Bases)\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Tools\\n  \\n\\n\\n\\n\\n\\n            Tools\\n          \\n\\n\\n\\n\\n    OnDemandLoaderTool Tutorial\\n  \\n\\n\\n\\n\\n\\n    Evaluation Query Engine Tool\\n  \\n\\n\\n\\n\\n\\n    None\\n  \\n\\n\\n\\n\\n\\n    Azure Code Interpreter Tool Spec\\n  \\n\\n\\n\\n\\n\\n    Cassandra Database Tools\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Transforms\\n  \\n\\n\\n\\n\\n\\n            Transforms\\n          \\n\\n\\n\\n\\n    Transforms Evaluation\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Use Cases\\n  \\n\\n\\n\\n\\n\\n            Use Cases\\n          \\n\\n\\n\\n\\n    10Q Analysis\\n  \\n\\n\\n\\n\\n\\n    10K Analysis\\n  \\n\\n\\n\\n\\n\\n    Github Issue Analysis\\n  \\n\\n\\n\\n\\n\\n    Email Data Extraction\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Vector Stores\\n  \\n\\n\\n\\n\\n\\n            Vector Stores\\n          \\n\\n\\n\\n\\n    Typesense Vector Store\\n  \\n\\n\\n\\n\\n\\n    Bagel Vector Store\\n  \\n\\n\\n\\n\\n\\n    Rockset Vector Store\\n  \\n\\n\\n\\n\\n\\n    Tencent Cloud VectorDB\\n  \\n\\n\\n\\n\\n\\n    Qdrant Vector Store\\n  \\n\\n\\n\\n\\n\\n    Timescale Vector Store (PostgreSQL)\\n  \\n\\n\\n\\n\\n\\n    MongoDBAtlasVectorSearch\\n  \\n\\n\\n\\n\\n\\n    DocArray InMemory Vector Store\\n  \\n\\n\\n\\n\\n\\n    Auto-Retrieval from a Vector Database\\n  \\n\\n\\n\\n\\n\\n    Zep Vector Store\\n  \\n\\n\\n\\n\\n\\n    Faiss Vector Store\\n  \\n\\n\\n\\n\\n\\n    Guide: Using Vector Store Index with Existing Pinecone Vector Store\\n  \\n\\n\\n\\n\\n\\n    Guide: Using Vector Store Index with Existing Weaviate Vector Store\\n  \\n\\n\\n\\n\\n\\n    Simple Vector Store\\n  \\n\\n\\n\\n\\n\\n    Qdrant Hybrid Search\\n  \\n\\n\\n\\n\\n\\n    Deep Lake Vector Store Quickstart\\n  \\n\\n\\n\\n\\n\\n    Pinecone Vector Store - Metadata Filter\\n  \\n\\n\\n\\n\\n\\n    Qdrant Vector Store - Default Qdrant Filters\\n  \\n\\n\\n\\n\\n\\n    Auto-Retrieval from a Vector Database\\n  \\n\\n\\n\\n\\n\\n    ClickHouse Vector Store\\n  \\n\\n\\n\\n\\n\\n    S3/R2 Storage\\n  \\n\\n\\n\\n\\n\\n    txtai Vector Store\\n  \\n\\n\\n\\n\\n\\n    Cassandra Vector Store\\n  \\n\\n\\n\\n\\n\\n    Elasticsearch\\n  \\n\\n\\n\\n\\n\\n    Awadb Vector Store\\n  \\n\\n\\n\\n\\n\\n    Postgres Vector Store\\n  \\n\\n\\n\\n\\n\\n    Chroma Vector Store\\n  \\n\\n\\n\\n\\n\\n    Azure CosmosDB MongoDB Vector Store\\n  \\n\\n\\n\\n\\n\\n    Upstash Vector Store\\n  \\n\\n\\n\\n\\n\\n    Neo4j vector store\\n  \\n\\n\\n\\n\\n\\n    Elasticsearch Vector Store\\n  \\n\\n\\n\\n\\n\\n    Local Llama2 + VectorStoreIndex\\n  \\n\\n\\n\\n\\n\\n    MyScale Vector Store\\n  \\n\\n\\n\\n\\n\\n    Metal Vector Store\\n  \\n\\n\\n\\n\\n\\n    Simple Vector Store - Async Index Creation\\n  \\n\\n\\n\\n\\n\\n    Tair Vector Store\\n  \\n\\n\\n\\n\\n\\n    Pinecone Vector Store\\n  \\n\\n\\n\\n\\n\\n    MongoDBAtlasVectorSearchRAGOpenAI\\n  \\n\\n\\n\\n\\n\\n    Redis Vector Store\\n  \\n\\n\\n\\n\\n\\n    Jaguar Vector Store\\n  \\n\\n\\n\\n\\n\\n    Llama2 + VectorStoreIndex\\n  \\n\\n\\n\\n\\n\\n    Weaviate Vector Store\\n  \\n\\n\\n\\n\\n\\n    Supabase Vector Store\\n  \\n\\n\\n\\n\\n\\n    pgvecto.rs\\n  \\n\\n\\n\\n\\n\\n    Weaviate Vector Store Metadata Filter\\n  \\n\\n\\n\\n\\n\\n    Weaviate Vector Store - Hybrid Search\\n  \\n\\n\\n\\n\\n\\n    DocArray Hnsw Vector Store\\n  \\n\\n\\n\\n\\n\\n    DashVector Vector Store\\n  \\n\\n\\n\\n\\n\\n    Opensearch Vector Store\\n  \\n\\n\\n\\n\\n\\n    Pinecone Vector Store - Hybrid Search\\n  \\n\\n\\n\\n\\n\\n    Qdrant Vector Store - Metadata Filter\\n  \\n\\n\\n\\n\\n\\n    Simple Vector Stores - Maximum Marginal Relevance Retrieval\\n  \\n\\n\\n\\n\\n\\n    A Simple to Advanced Guide with Auto-Retrieval (with Pinecone + Arize Phoenix)\\n  \\n\\n\\n\\n\\n\\n    Chroma\\n  \\n\\n\\n\\n\\n\\n    LanceDB Vector Store\\n  \\n\\n\\n\\n\\n\\n    Bagel Network\\n  \\n\\n\\n\\n\\n\\n    Epsilla Vector Store\\n  \\n\\n\\n\\n\\n\\n    Milvus Vector Store\\n  \\n\\n\\n\\n\\n\\n    Azure AI Search\\n  \\n\\n\\n\\n\\n\\n    Lantern Vector Store\\n  \\n\\n\\n\\n\\n\\n    Astra DB\\n  \\n\\n\\n\\n\\n\\n    Lantern Vector Store (auto-retriever)\\n  \\n\\n\\n\\n\\n\\n    Auto-Retrieval from a Weaviate Vector Database\\n  \\n\\n\\n\\n\\n\\n    Databricks Vector Search\\n  \\n\\n\\n\\n\\n\\n    Chroma + Fireworks + Nomic with Matryoshka embedding\\n  \\n\\n\\n\\n\\n\\n    DuckDB\\n  \\n\\n\\n\\n\\n\\n    Baidu VectorDB\\n  \\n\\n\\n\\n\\n\\n    now make sure you create the search index with the right name here\\n  \\n\\n\\n\\n\\n\\n    Advanced RAG with temporal filters using LlamaIndex and KDB.AI vector store\\n  \\n\\n\\n\\n\\n\\n    AnalyticDB\\n  \\n\\n\\n\\n\\n\\n    TiDB Vector Store\\n  \\n\\n\\n\\n\\n\\n    Amazon Neptune - Neptune Analytics vector store\\n  \\n\\n\\n\\n\\n\\n    CouchbaseVectorStoreDemo\\n  \\n\\n\\n\\n\\n\\n    VearchDemo\\n  \\n\\n\\n\\n\\n\\n    Neo4j Vector Store - Metadata Filter\\n  \\n\\n\\n\\n\\n\\n    AWSDocDBDemo\\n  \\n\\n\\n\\n\\n\\n    Milvus Vector Store With Hybrid Retrieval\\n  \\n\\n\\n\\n\\n\\n    Firestore Vector Store\\n  \\n\\n\\n\\n\\n\\n    Vespa Vector Store demo\\n  \\n\\n\\n\\n\\n\\n    Google Vertex AI Vector Search\\n  \\n\\n\\n\\n\\n\\n    Alibaba Cloud OpenSearch Vector Store\\n  \\n\\n\\n\\n\\n\\n    Relyt\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Component Guides\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Component Guides\\n          \\n\\n\\n\\n\\n\\n\\n    Models\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Models\\n          \\n\\n\\n\\n\\n\\n    LLMs\\n  \\n\\n\\n\\n\\n\\n            LLMs\\n          \\n\\n\\n\\n\\n    Using LLMs\\n  \\n\\n\\n\\n\\n\\n    Standalone Usage\\n  \\n\\n\\n\\n\\n\\n    Customizing LLMs\\n  \\n\\n\\n\\n\\n\\n    Available LLM Integrations\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Embeddings\\n  \\n\\n\\n\\n\\n\\n    Multi Modal\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Prompts\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Prompts\\n          \\n\\n\\n\\n\\n    Usage pattern\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Loading\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Loading\\n          \\n\\n\\n\\n\\n\\n\\n    Documents and Nodes\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Documents and Nodes\\n          \\n\\n\\n\\n\\n    Using Documents\\n  \\n\\n\\n\\n\\n\\n    Using Nodes\\n  \\n\\n\\n\\n\\n\\n    Metadata Extraction\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    SimpleDirectoryReader\\n  \\n\\n\\n\\n\\n\\n\\n\\n    Data Connectors\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Data Connectors\\n          \\n\\n\\n\\n\\n    Usage Pattern\\n  \\n\\n\\n\\n\\n\\n    LlamaParse\\n  \\n\\n\\n\\n\\n\\n    Module Guides\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Node Parsers / Text Splitters\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Node Parsers / Text Splitters\\n          \\n\\n\\n\\n\\n    Node Parser Modules\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Ingestion Pipeline\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Ingestion Pipeline\\n          \\n\\n\\n\\n\\n    Transformations\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Indexing\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Indexing\\n          \\n\\n\\n\\n\\n    Index Guide\\n  \\n\\n\\n\\n\\n\\n    Vector Store Index\\n  \\n\\n\\n\\n\\n\\n    Document Management\\n  \\n\\n\\n\\n\\n\\n    LlamaCloud\\n  \\n\\n\\n\\n\\n\\n    Metadata Extraction\\n  \\n\\n\\n\\n\\n\\n    Modules\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Storing\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Storing\\n          \\n\\n\\n\\n\\n    Vector Stores\\n  \\n\\n\\n\\n\\n\\n    Document Stores\\n  \\n\\n\\n\\n\\n\\n    Index Stores\\n  \\n\\n\\n\\n\\n\\n    Chat Stores\\n  \\n\\n\\n\\n\\n\\n    Key-Value Stores\\n  \\n\\n\\n\\n\\n\\n    Persisting & Loading Data\\n  \\n\\n\\n\\n\\n\\n    Customizing Storage\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Querying\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Querying\\n          \\n\\n\\n\\n\\n\\n\\n    Query Engines\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Query Engines\\n          \\n\\n\\n\\n\\n    Usage Pattern\\n  \\n\\n\\n\\n\\n\\n    Response Modes\\n  \\n\\n\\n\\n\\n\\n    Streaming\\n  \\n\\n\\n\\n\\n\\n    Module Guides\\n  \\n\\n\\n\\n\\n\\n    Supporting Modules\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Chat Engines\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Chat Engines\\n          \\n\\n\\n\\n\\n    Usage Pattern\\n  \\n\\n\\n\\n\\n\\n    Module Guides\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Retrieval\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Retrieval\\n          \\n\\n\\n\\n\\n    Retriever Modules\\n  \\n\\n\\n\\n\\n\\n    Retriever Modes\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Node Postprocessors\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Node Postprocessors\\n          \\n\\n\\n\\n\\n    Node Postprocessor Modules\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Response Synthesis\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Response Synthesis\\n          \\n\\n\\n\\n\\n    Response Synthesis Modules\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Routing\\n  \\n\\n\\n\\n\\n\\n\\n\\n    Query Pipelines\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Query Pipelines\\n          \\n\\n\\n\\n\\n    Usage Pattern\\n  \\n\\n\\n\\n\\n\\n    Module Guides\\n  \\n\\n\\n\\n\\n\\n    Module Usage\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Structured Outputs\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Structured Outputs\\n          \\n\\n\\n\\n\\n    Output Parsing Modules\\n  \\n\\n\\n\\n\\n\\n    Query Engines + Pydantic Outputs\\n  \\n\\n\\n\\n\\n\\n    Pydantic Program\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Agents\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Agents\\n          \\n\\n\\n\\n\\n    Usage Pattern\\n  \\n\\n\\n\\n\\n\\n    Lower-Level Agent API\\n  \\n\\n\\n\\n\\n\\n    Module Guides\\n  \\n\\n\\n\\n\\n\\n    Tools\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Evaluation\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Evaluation\\n          \\n\\n\\n\\n\\n    Usage Pattern (Response Evaluation)\\n  \\n\\n\\n\\n\\n\\n    Usage Pattern (Retrieval)\\n  \\n\\n\\n\\n\\n\\n    Modules\\n  \\n\\n\\n\\n\\n\\n\\n    LlamaDatasets\\n  \\n\\n\\n\\n\\n\\n            LlamaDatasets\\n          \\n\\n\\n\\n\\n    Contributing A LabelledRagDataset\\n  \\n\\n\\n\\n\\n\\n    Evaluating With LabelledRagDataset\\'s\\n  \\n\\n\\n\\n\\n\\n    Evaluating Evaluators with LabelledEvaluatorDataset\\'s\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Observability\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Observability\\n          \\n\\n\\n\\n\\n    Instrumentation\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Settings\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Advanced Topics\\n  \\n\\n\\n\\n\\n\\n            Advanced Topics\\n          \\n\\n\\n\\n\\n    Building Performant RAG Applications for Production\\n  \\n\\n\\n\\n\\n\\n    Basic Strategies\\n  \\n\\n\\n\\n\\n\\n    Agentic strategies\\n  \\n\\n\\n\\n\\n\\n\\n    Retrieval\\n  \\n\\n\\n\\n\\n\\n            Retrieval\\n          \\n\\n\\n\\n\\n    Advanced Retrieval Strategies\\n  \\n\\n\\n\\n\\n\\n    Query Transformations\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Evaluation\\n  \\n\\n\\n\\n\\n\\n            Evaluation\\n          \\n\\n\\n\\n\\n    Component Wise Evaluation\\n  \\n\\n\\n\\n\\n\\n    End-to-End Evaluation\\n  \\n\\n\\n\\n\\n\\n    Evaluation\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Fine-Tuning\\n  \\n\\n\\n\\n\\n\\n    Writing Custom Modules\\n  \\n\\n\\n\\n\\n\\n    Building RAG from Scratch (Lower-Level)\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    API Reference\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            API Reference\\n          \\n\\n\\n\\n\\n\\n\\n    Agents\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Agents\\n          \\n\\n\\n\\n\\n    Coa\\n  \\n\\n\\n\\n\\n\\n    Introspective\\n  \\n\\n\\n\\n\\n\\n    Lats\\n  \\n\\n\\n\\n\\n\\n    Llm compiler\\n  \\n\\n\\n\\n\\n\\n    Openai\\n  \\n\\n\\n\\n\\n\\n    Openai legacy\\n  \\n\\n\\n\\n\\n\\n    React\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Callbacks\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Callbacks\\n          \\n\\n\\n\\n\\n    Aim\\n  \\n\\n\\n\\n\\n\\n    Argilla\\n  \\n\\n\\n\\n\\n\\n    Arize phoenix\\n  \\n\\n\\n\\n\\n\\n    Deepeval\\n  \\n\\n\\n\\n\\n\\n    Honeyhive\\n  \\n\\n\\n\\n\\n\\n    Langfuse\\n  \\n\\n\\n\\n\\n\\n    Llama debug\\n  \\n\\n\\n\\n\\n\\n    Openinference\\n  \\n\\n\\n\\n\\n\\n    Promptlayer\\n  \\n\\n\\n\\n\\n\\n    Token counter\\n  \\n\\n\\n\\n\\n\\n    Uptrain\\n  \\n\\n\\n\\n\\n\\n    Wandb\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Chat Engines\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Chat Engines\\n          \\n\\n\\n\\n\\n    Condense plus context\\n  \\n\\n\\n\\n\\n\\n    Condense question\\n  \\n\\n\\n\\n\\n\\n    Context\\n  \\n\\n\\n\\n\\n\\n    Simple\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Embeddings\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Embeddings\\n          \\n\\n\\n\\n\\n    Adapter\\n  \\n\\n\\n\\n\\n\\n    Alephalpha\\n  \\n\\n\\n\\n\\n\\n    Anyscale\\n  \\n\\n\\n\\n\\n\\n    Azure openai\\n  \\n\\n\\n\\n\\n\\n    Bedrock\\n  \\n\\n\\n\\n\\n\\n    Clarifai\\n  \\n\\n\\n\\n\\n\\n    Clip\\n  \\n\\n\\n\\n\\n\\n    Cloudflare workersai\\n  \\n\\n\\n\\n\\n\\n    Cohere\\n  \\n\\n\\n\\n\\n\\n    Dashscope\\n  \\n\\n\\n\\n\\n\\n    Deepinfra\\n  \\n\\n\\n\\n\\n\\n    Elasticsearch\\n  \\n\\n\\n\\n\\n\\n    Fastembed\\n  \\n\\n\\n\\n\\n\\n    Fireworks\\n  \\n\\n\\n\\n\\n\\n    Gemini\\n  \\n\\n\\n\\n\\n\\n    Google\\n  \\n\\n\\n\\n\\n\\n    Gradient\\n  \\n\\n\\n\\n\\n\\n    Huggingface\\n  \\n\\n\\n\\n\\n\\n    Huggingface itrex\\n  \\n\\n\\n\\n\\n\\n    Huggingface openvino\\n  \\n\\n\\n\\n\\n\\n    Huggingface optimum\\n  \\n\\n\\n\\n\\n\\n    Huggingface optimum intel\\n  \\n\\n\\n\\n\\n\\n    Instructor\\n  \\n\\n\\n\\n\\n\\n    Ipex llm\\n  \\n\\n\\n\\n\\n\\n    Jinaai\\n  \\n\\n\\n\\n\\n\\n    Langchain\\n  \\n\\n\\n\\n\\n\\n    Llamafile\\n  \\n\\n\\n\\n\\n\\n    Llm rails\\n  \\n\\n\\n\\n\\n\\n    Mistralai\\n  \\n\\n\\n\\n\\n\\n    Nomic\\n  \\n\\n\\n\\n\\n\\n    Nvidia\\n  \\n\\n\\n\\n\\n\\n    Octoai\\n  \\n\\n\\n\\n\\n\\n    Ollama\\n  \\n\\n\\n\\n\\n\\n    Openai\\n  \\n\\n\\n\\n\\n\\n    Premai\\n  \\n\\n\\n\\n\\n\\n    Sagemaker endpoint\\n  \\n\\n\\n\\n\\n\\n    Text embeddings inference\\n  \\n\\n\\n\\n\\n\\n    Together\\n  \\n\\n\\n\\n\\n\\n    Upstage\\n  \\n\\n\\n\\n\\n\\n    Vertex\\n  \\n\\n\\n\\n\\n\\n    Voyageai\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Evaluation\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Evaluation\\n          \\n\\n\\n\\n\\n    Answer relevancy\\n  \\n\\n\\n\\n\\n\\n    Context relevancy\\n  \\n\\n\\n\\n\\n\\n    Correctness\\n  \\n\\n\\n\\n\\n\\n    Dataset generation\\n  \\n\\n\\n\\n\\n\\n    Faithfullness\\n  \\n\\n\\n\\n\\n\\n    Guideline\\n  \\n\\n\\n\\n\\n\\n    Metrics\\n  \\n\\n\\n\\n\\n\\n    Multi modal\\n  \\n\\n\\n\\n\\n\\n    Pairwise comparison\\n  \\n\\n\\n\\n\\n\\n    Query response\\n  \\n\\n\\n\\n\\n\\n    Response\\n  \\n\\n\\n\\n\\n\\n    Retrieval\\n  \\n\\n\\n\\n\\n\\n    Semantic similarity\\n  \\n\\n\\n\\n\\n\\n    Tonic validate\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Indexes\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Indexes\\n          \\n\\n\\n\\n\\n    Colbert\\n  \\n\\n\\n\\n\\n\\n    Document summary\\n  \\n\\n\\n\\n\\n\\n    Google\\n  \\n\\n\\n\\n\\n\\n    Keyword\\n  \\n\\n\\n\\n\\n\\n    Knowledge graph\\n  \\n\\n\\n\\n\\n\\n    Llama cloud\\n  \\n\\n\\n\\n\\n\\n    Postgresml\\n  \\n\\n\\n\\n\\n\\n    Summary\\n  \\n\\n\\n\\n\\n\\n    Tree\\n  \\n\\n\\n\\n\\n\\n    Vectara\\n  \\n\\n\\n\\n\\n\\n    Vector\\n  \\n\\n\\n\\n\\n\\n    Zilliz\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Ingestion\\n  \\n\\n\\n\\n\\n\\n            Ingestion\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Instrumentation\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Instrumentation\\n          \\n\\n\\n\\n\\n    Event handlers\\n  \\n\\n\\n\\n\\n\\n    Event types\\n  \\n\\n\\n\\n\\n\\n    Span handlers\\n  \\n\\n\\n\\n\\n\\n    Span types\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LLMs\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            LLMs\\n          \\n\\n\\n\\n\\n    Ai21\\n  \\n\\n\\n\\n\\n\\n    Alephalpha\\n  \\n\\n\\n\\n\\n\\n    Anthropic\\n  \\n\\n\\n\\n\\n\\n    Anyscale\\n  \\n\\n\\n\\n\\n\\n    Azure openai\\n  \\n\\n\\n\\n\\n\\n    Bedrock\\n  \\n\\n\\n\\n\\n\\n    Clarifai\\n  \\n\\n\\n\\n\\n\\n    Cohere\\n  \\n\\n\\n\\n\\n\\n    Custom llm\\n  \\n\\n\\n\\n\\n\\n    Dashscope\\n  \\n\\n\\n\\n\\n\\n    Databricks\\n  \\n\\n\\n\\n\\n\\n    Deepinfra\\n  \\n\\n\\n\\n\\n\\n    Everlyai\\n  \\n\\n\\n\\n\\n\\n    Fireworks\\n  \\n\\n\\n\\n\\n\\n    Friendli\\n  \\n\\n\\n\\n\\n\\n    Gemini\\n  \\n\\n\\n\\n\\n\\n    Gradient\\n  \\n\\n\\n\\n\\n\\n    Groq\\n  \\n\\n\\n\\n\\n\\n    Huggingface\\n  \\n\\n\\n\\n\\n\\n    Ipex llm\\n  \\n\\n\\n\\n\\n\\n    Konko\\n  \\n\\n\\n\\n\\n\\n    Langchain\\n  \\n\\n\\n\\n\\n\\n    Litellm\\n  \\n\\n\\n\\n\\n\\n    Llama api\\n  \\n\\n\\n\\n\\n\\n    Llama cpp\\n  \\n\\n\\n\\n\\n\\n    Llamafile\\n  \\n\\n\\n\\n\\n\\n    Lmstudio\\n  \\n\\n\\n\\n\\n\\n    Localai\\n  \\n\\n\\n\\n\\n\\n    Maritalk\\n  \\n\\n\\n\\n\\n\\n    Mistral rs\\n  \\n\\n\\n\\n\\n\\n    Mistralai\\n  \\n\\n\\n\\n\\n\\n    Mlx\\n  \\n\\n\\n\\n\\n\\n    Modelscope\\n  \\n\\n\\n\\n\\n\\n    Monsterapi\\n  \\n\\n\\n\\n\\n\\n    Mymagic\\n  \\n\\n\\n\\n\\n\\n    Neutrino\\n  \\n\\n\\n\\n\\n\\n    Nvidia\\n  \\n\\n\\n\\n\\n\\n    Nvidia tensorrt\\n  \\n\\n\\n\\n\\n\\n    Nvidia triton\\n  \\n\\n\\n\\n\\n\\n    Octoai\\n  \\n\\n\\n\\n\\n\\n    Ollama\\n  \\n\\n\\n\\n\\n\\n    Openai\\n  \\n\\n\\n\\n\\n\\n    Openai like\\n  \\n\\n\\n\\n\\n\\n    Openllm\\n  \\n\\n\\n\\n\\n\\n    Openrouter\\n  \\n\\n\\n\\n\\n\\n    Openvino\\n  \\n\\n\\n\\n\\n\\n    Palm\\n  \\n\\n\\n\\n\\n\\n    Perplexity\\n  \\n\\n\\n\\n\\n\\n    Portkey\\n  \\n\\n\\n\\n\\n\\n    Predibase\\n  \\n\\n\\n\\n\\n\\n    Premai\\n  \\n\\n\\n\\n\\n\\n    Replicate\\n  \\n\\n\\n\\n\\n\\n    Rungpt\\n  \\n\\n\\n\\n\\n\\n    Sagemaker endpoint\\n  \\n\\n\\n\\n\\n\\n    Solar\\n  \\n\\n\\n\\n\\n\\n    Together\\n  \\n\\n\\n\\n\\n\\n    Unify\\n  \\n\\n\\n\\n\\n\\n    Upstage\\n  \\n\\n\\n\\n\\n\\n    Vertex\\n  \\n\\n\\n\\n\\n\\n    Vllm\\n  \\n\\n\\n\\n\\n\\n    Watsonx\\n  \\n\\n\\n\\n\\n\\n    Xinference\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Llama Datasets\\n  \\n\\n\\n\\n\\n\\n            Llama Datasets\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Llama Packs\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Llama Packs\\n          \\n\\n\\n\\n\\n    Agent search retriever\\n  \\n\\n\\n\\n\\n\\n    Agents coa\\n  \\n\\n\\n\\n\\n\\n    Agents lats\\n  \\n\\n\\n\\n\\n\\n    Agents llm compiler\\n  \\n\\n\\n\\n\\n\\n    Amazon product extraction\\n  \\n\\n\\n\\n\\n\\n    Arize phoenix query engine\\n  \\n\\n\\n\\n\\n\\n    Auto merging retriever\\n  \\n\\n\\n\\n\\n\\n    Chroma autoretrieval\\n  \\n\\n\\n\\n\\n\\n    Code hierarchy\\n  \\n\\n\\n\\n\\n\\n    Cogniswitch agent\\n  \\n\\n\\n\\n\\n\\n    Cohere citation chat\\n  \\n\\n\\n\\n\\n\\n    Corrective rag\\n  \\n\\n\\n\\n\\n\\n    Deeplake deepmemory retriever\\n  \\n\\n\\n\\n\\n\\n    Deeplake multimodal retrieval\\n  \\n\\n\\n\\n\\n\\n    Dense x retrieval\\n  \\n\\n\\n\\n\\n\\n    Diff private simple dataset\\n  \\n\\n\\n\\n\\n\\n    Docugami kg rag\\n  \\n\\n\\n\\n\\n\\n    Evaluator benchmarker\\n  \\n\\n\\n\\n\\n\\n    Finchat\\n  \\n\\n\\n\\n\\n\\n    Fusion retriever\\n  \\n\\n\\n\\n\\n\\n    Fuzzy citation\\n  \\n\\n\\n\\n\\n\\n    Gmail openai agent\\n  \\n\\n\\n\\n\\n\\n    Gradio agent chat\\n  \\n\\n\\n\\n\\n\\n    Gradio react agent chatbot\\n  \\n\\n\\n\\n\\n\\n    Infer retrieve rerank\\n  \\n\\n\\n\\n\\n\\n    Koda retriever\\n  \\n\\n\\n\\n\\n\\n    Llama dataset metadata\\n  \\n\\n\\n\\n\\n\\n    Llama guard moderator\\n  \\n\\n\\n\\n\\n\\n    Llava completion\\n  \\n\\n\\n\\n\\n\\n    Multi document agents\\n  \\n\\n\\n\\n\\n\\n    Multi tenancy rag\\n  \\n\\n\\n\\n\\n\\n    Multidoc autoretrieval\\n  \\n\\n\\n\\n\\n\\n    Nebulagraph query engine\\n  \\n\\n\\n\\n\\n\\n    Neo4j query engine\\n  \\n\\n\\n\\n\\n\\n    Node parser semantic chunking\\n  \\n\\n\\n\\n\\n\\n    Ollama query engine\\n  \\n\\n\\n\\n\\n\\n    Panel chatbot\\n  \\n\\n\\n\\n\\n\\n    Query understanding agent\\n  \\n\\n\\n\\n\\n\\n    Raft dataset\\n  \\n\\n\\n\\n\\n\\n    Rag cli local\\n  \\n\\n\\n\\n\\n\\n    Rag evaluator\\n  \\n\\n\\n\\n\\n\\n    Rag fusion query pipeline\\n  \\n\\n\\n\\n\\n\\n    Ragatouille retriever\\n  \\n\\n\\n\\n\\n\\n    Raptor\\n  \\n\\n\\n\\n\\n\\n    Recursive retriever\\n  \\n\\n\\n\\n\\n\\n    Redis ingestion pipeline\\n  \\n\\n\\n\\n\\n\\n    Resume screener\\n  \\n\\n\\n\\n\\n\\n    Retry engine weaviate\\n  \\n\\n\\n\\n\\n\\n    Searchain\\n  \\n\\n\\n\\n\\n\\n    Self discover\\n  \\n\\n\\n\\n\\n\\n    Self rag\\n  \\n\\n\\n\\n\\n\\n    Sentence window retriever\\n  \\n\\n\\n\\n\\n\\n    Snowflake query engine\\n  \\n\\n\\n\\n\\n\\n    Stock market data query engine\\n  \\n\\n\\n\\n\\n\\n    Streamlit chatbot\\n  \\n\\n\\n\\n\\n\\n    Sub question weaviate\\n  \\n\\n\\n\\n\\n\\n    Subdoc summary\\n  \\n\\n\\n\\n\\n\\n    Tables\\n  \\n\\n\\n\\n\\n\\n    Timescale vector autoretrieval\\n  \\n\\n\\n\\n\\n\\n    Trulens eval packs\\n  \\n\\n\\n\\n\\n\\n    Vanna\\n  \\n\\n\\n\\n\\n\\n    Vectara rag\\n  \\n\\n\\n\\n\\n\\n    Voyage query engine\\n  \\n\\n\\n\\n\\n\\n    Zephyr query engine\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Memory\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Memory\\n          \\n\\n\\n\\n\\n    Chat memory buffer\\n  \\n\\n\\n\\n\\n\\n    Simple composable memory\\n  \\n\\n\\n\\n\\n\\n    Vector memory\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Metadata Extractors\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Metadata Extractors\\n          \\n\\n\\n\\n\\n    Entity\\n  \\n\\n\\n\\n\\n\\n    Keyword\\n  \\n\\n\\n\\n\\n\\n    Marvin\\n  \\n\\n\\n\\n\\n\\n    Pydantic\\n  \\n\\n\\n\\n\\n\\n    Question\\n  \\n\\n\\n\\n\\n\\n    Summary\\n  \\n\\n\\n\\n\\n\\n    Title\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Multi-Modal LLMs\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Multi-Modal LLMs\\n          \\n\\n\\n\\n\\n    Anthropic\\n  \\n\\n\\n\\n\\n\\n    Azure openai\\n  \\n\\n\\n\\n\\n\\n    Dashscope\\n  \\n\\n\\n\\n\\n\\n    Gemini\\n  \\n\\n\\n\\n\\n\\n    Ollama\\n  \\n\\n\\n\\n\\n\\n    Openai\\n  \\n\\n\\n\\n\\n\\n    Replicate\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Node Parsers & Text Splitters\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Node Parsers & Text Splitters\\n          \\n\\n\\n\\n\\n    Code\\n  \\n\\n\\n\\n\\n\\n    Hierarchical\\n  \\n\\n\\n\\n\\n\\n    Html\\n  \\n\\n\\n\\n\\n\\n    Json\\n  \\n\\n\\n\\n\\n\\n    Langchain\\n  \\n\\n\\n\\n\\n\\n    Markdown\\n  \\n\\n\\n\\n\\n\\n    Markdown element\\n  \\n\\n\\n\\n\\n\\n    Semantic splitter\\n  \\n\\n\\n\\n\\n\\n    Sentence splitter\\n  \\n\\n\\n\\n\\n\\n    Sentence window\\n  \\n\\n\\n\\n\\n\\n    Token text splitter\\n  \\n\\n\\n\\n\\n\\n    Unstructured element\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Node Postprocessors\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Node Postprocessors\\n          \\n\\n\\n\\n\\n    NER PII\\n  \\n\\n\\n\\n\\n\\n    PII\\n  \\n\\n\\n\\n\\n\\n    Auto prev next\\n  \\n\\n\\n\\n\\n\\n    Cohere rerank\\n  \\n\\n\\n\\n\\n\\n    Colbert rerank\\n  \\n\\n\\n\\n\\n\\n    Dashscope rerank\\n  \\n\\n\\n\\n\\n\\n    Embedding recency\\n  \\n\\n\\n\\n\\n\\n    Fixed recency\\n  \\n\\n\\n\\n\\n\\n    Flag embedding reranker\\n  \\n\\n\\n\\n\\n\\n    Jinaai rerank\\n  \\n\\n\\n\\n\\n\\n    Keyword\\n  \\n\\n\\n\\n\\n\\n    Llm rerank\\n  \\n\\n\\n\\n\\n\\n    Long context reorder\\n  \\n\\n\\n\\n\\n\\n    Longllmlingua\\n  \\n\\n\\n\\n\\n\\n    Metadata replacement\\n  \\n\\n\\n\\n\\n\\n    Nvidia rerank\\n  \\n\\n\\n\\n\\n\\n    Openvino rerank\\n  \\n\\n\\n\\n\\n\\n    Presidio\\n  \\n\\n\\n\\n\\n\\n    Prev next\\n  \\n\\n\\n\\n\\n\\n    Rankgpt rerank\\n  \\n\\n\\n\\n\\n\\n    Rankllm rerank\\n  \\n\\n\\n\\n\\n\\n    Sbert rerank\\n  \\n\\n\\n\\n\\n\\n    Sentence optimizer\\n  \\n\\n\\n\\n\\n\\n    Similarity\\n  \\n\\n\\n\\n\\n\\n    Time weighted\\n  \\n\\n\\n\\n\\n\\n    Voyageai rerank\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Object Stores\\n  \\n\\n\\n\\n\\n\\n            Object Stores\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Output Parsers\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Output Parsers\\n          \\n\\n\\n\\n\\n    Guardrails\\n  \\n\\n\\n\\n\\n\\n    Langchain\\n  \\n\\n\\n\\n\\n\\n    Pydantic\\n  \\n\\n\\n\\n\\n\\n    Selection\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Programs\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Programs\\n          \\n\\n\\n\\n\\n    Evaporate\\n  \\n\\n\\n\\n\\n\\n    Guidance\\n  \\n\\n\\n\\n\\n\\n    Llm text completion\\n  \\n\\n\\n\\n\\n\\n    Lmformatenforcer\\n  \\n\\n\\n\\n\\n\\n    Multi modal\\n  \\n\\n\\n\\n\\n\\n    Openai\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Prompts\\n  \\n\\n\\n\\n\\n\\n            Prompts\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Query Engines\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Query Engines\\n          \\n\\n\\n\\n\\n    FLARE\\n  \\n\\n\\n\\n\\n\\n    JSONalayze\\n  \\n\\n\\n\\n\\n\\n    NL SQL table\\n  \\n\\n\\n\\n\\n\\n    PGVector SQL\\n  \\n\\n\\n\\n\\n\\n    SQL join\\n  \\n\\n\\n\\n\\n\\n    SQL table retriever\\n  \\n\\n\\n\\n\\n\\n    Citation\\n  \\n\\n\\n\\n\\n\\n    Cogniswitch\\n  \\n\\n\\n\\n\\n\\n    Custom\\n  \\n\\n\\n\\n\\n\\n    Knowledge graph\\n  \\n\\n\\n\\n\\n\\n    Multi step\\n  \\n\\n\\n\\n\\n\\n    Pandas\\n  \\n\\n\\n\\n\\n\\n    Retriever\\n  \\n\\n\\n\\n\\n\\n    Retriever router\\n  \\n\\n\\n\\n\\n\\n    Retry\\n  \\n\\n\\n\\n\\n\\n    Router\\n  \\n\\n\\n\\n\\n\\n    Simple multi modal\\n  \\n\\n\\n\\n\\n\\n    Sub question\\n  \\n\\n\\n\\n\\n\\n    Tool retriever router\\n  \\n\\n\\n\\n\\n\\n    Transform\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Query Pipeline\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Query Pipeline\\n          \\n\\n\\n\\n\\n    Agent\\n  \\n\\n\\n\\n\\n\\n    Arg pack\\n  \\n\\n\\n\\n\\n\\n    Custom\\n  \\n\\n\\n\\n\\n\\n    Function\\n  \\n\\n\\n\\n\\n\\n    Input\\n  \\n\\n\\n\\n\\n\\n    Llm\\n  \\n\\n\\n\\n\\n\\n    Multi modal\\n  \\n\\n\\n\\n\\n\\n    Object\\n  \\n\\n\\n\\n\\n\\n    Output parser\\n  \\n\\n\\n\\n\\n\\n    Postprocessor\\n  \\n\\n\\n\\n\\n\\n    Prompt\\n  \\n\\n\\n\\n\\n\\n    Query engine\\n  \\n\\n\\n\\n\\n\\n    Query transform\\n  \\n\\n\\n\\n\\n\\n    Retriever\\n  \\n\\n\\n\\n\\n\\n    Router\\n  \\n\\n\\n\\n\\n\\n    Synthesizer\\n  \\n\\n\\n\\n\\n\\n    Tool runner\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Question Generators\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Question Generators\\n          \\n\\n\\n\\n\\n    Guidance\\n  \\n\\n\\n\\n\\n\\n    Llm question gen\\n  \\n\\n\\n\\n\\n\\n    Openai\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Readers\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Readers\\n          \\n\\n\\n\\n\\n    Agent search\\n  \\n\\n\\n\\n\\n\\n    Airbyte cdk\\n  \\n\\n\\n\\n\\n\\n    Airbyte gong\\n  \\n\\n\\n\\n\\n\\n    Airbyte hubspot\\n  \\n\\n\\n\\n\\n\\n    Airbyte salesforce\\n  \\n\\n\\n\\n\\n\\n    Airbyte shopify\\n  \\n\\n\\n\\n\\n\\n    Airbyte stripe\\n  \\n\\n\\n\\n\\n\\n    Airbyte typeform\\n  \\n\\n\\n\\n\\n\\n    Airbyte zendesk support\\n  \\n\\n\\n\\n\\n\\n    Airtable\\n  \\n\\n\\n\\n\\n\\n    Apify\\n  \\n\\n\\n\\n\\n\\n    Arango db\\n  \\n\\n\\n\\n\\n\\n    Arxiv\\n  \\n\\n\\n\\n\\n\\n    Asana\\n  \\n\\n\\n\\n\\n\\n    Assemblyai\\n  \\n\\n\\n\\n\\n\\n    Astra db\\n  \\n\\n\\n\\n\\n\\n    Athena\\n  \\n\\n\\n\\n\\n\\n    Awadb\\n  \\n\\n\\n\\n\\n\\n    Azcognitive search\\n  \\n\\n\\n\\n\\n\\n    Azstorage blob\\n  \\n\\n\\n\\n\\n\\n    Bagel\\n  \\n\\n\\n\\n\\n\\n    Bilibili\\n  \\n\\n\\n\\n\\n\\n    Bitbucket\\n  \\n\\n\\n\\n\\n\\n    Boarddocs\\n  \\n\\n\\n\\n\\n\\n    Chatgpt plugin\\n  \\n\\n\\n\\n\\n\\n    Chroma\\n  \\n\\n\\n\\n\\n\\n    Clickhouse\\n  \\n\\n\\n\\n\\n\\n    Confluence\\n  \\n\\n\\n\\n\\n\\n    Couchbase\\n  \\n\\n\\n\\n\\n\\n    Couchdb\\n  \\n\\n\\n\\n\\n\\n    Dad jokes\\n  \\n\\n\\n\\n\\n\\n    Dashvector\\n  \\n\\n\\n\\n\\n\\n    Database\\n  \\n\\n\\n\\n\\n\\n    Deeplake\\n  \\n\\n\\n\\n\\n\\n    Discord\\n  \\n\\n\\n\\n\\n\\n    Docstring walker\\n  \\n\\n\\n\\n\\n\\n    Docugami\\n  \\n\\n\\n\\n\\n\\n    Earnings call transcript\\n  \\n\\n\\n\\n\\n\\n    Elasticsearch\\n  \\n\\n\\n\\n\\n\\n    Faiss\\n  \\n\\n\\n\\n\\n\\n    Feedly rss\\n  \\n\\n\\n\\n\\n\\n    Feishu docs\\n  \\n\\n\\n\\n\\n\\n    Feishu wiki\\n  \\n\\n\\n\\n\\n\\n    File\\n  \\n\\n\\n\\n\\n\\n    Firebase realtimedb\\n  \\n\\n\\n\\n\\n\\n    Firestore\\n  \\n\\n\\n\\n\\n\\n    Gcs\\n  \\n\\n\\n\\n\\n\\n    Genius\\n  \\n\\n\\n\\n\\n\\n    Github\\n  \\n\\n\\n\\n\\n\\n    Google\\n  \\n\\n\\n\\n\\n\\n    Gpt repo\\n  \\n\\n\\n\\n\\n\\n    Graphdb cypher\\n  \\n\\n\\n\\n\\n\\n    Graphql\\n  \\n\\n\\n\\n\\n\\n    Guru\\n  \\n\\n\\n\\n\\n\\n    Hatena blog\\n  \\n\\n\\n\\n\\n\\n    Hive\\n  \\n\\n\\n\\n\\n\\n    Hubspot\\n  \\n\\n\\n\\n\\n\\n    Huggingface fs\\n  \\n\\n\\n\\n\\n\\n    Hwp\\n  \\n\\n\\n\\n\\n\\n    Imdb review\\n  \\n\\n\\n\\n\\n\\n    Intercom\\n  \\n\\n\\n\\n\\n\\n    Jaguar\\n  \\n\\n\\n\\n\\n\\n    Jira\\n  \\n\\n\\n\\n\\n\\n    Joplin\\n  \\n\\n\\n\\n\\n\\n    Json\\n  \\n\\n\\n\\n\\n\\n    Kaltura esearch\\n  \\n\\n\\n\\n\\n\\n    Kibela\\n  \\n\\n\\n\\n\\n\\n    Lilac\\n  \\n\\n\\n\\n\\n\\n    Linear\\n  \\n\\n\\n\\n\\n\\n    Llama parse\\n  \\n\\n\\n\\n\\n\\n    Macrometa gdn\\n  \\n\\n\\n\\n\\n\\n    Make com\\n  \\n\\n\\n\\n\\n\\n    Mangadex\\n  \\n\\n\\n\\n\\n\\n    Mangoapps guides\\n  \\n\\n\\n\\n\\n\\n    Maps\\n  \\n\\n\\n\\n\\n\\n    Mbox\\n  \\n\\n\\n\\n\\n\\n    Memos\\n  \\n\\n\\n\\n\\n\\n    Metal\\n  \\n\\n\\n\\n\\n\\n    Microsoft onedrive\\n  \\n\\n\\n\\n\\n\\n    Microsoft outlook\\n  \\n\\n\\n\\n\\n\\n    Microsoft sharepoint\\n  \\n\\n\\n\\n\\n\\n    Milvus\\n  \\n\\n\\n\\n\\n\\n    Minio\\n  \\n\\n\\n\\n\\n\\n    Mondaydotcom\\n  \\n\\n\\n\\n\\n\\n    Mongodb\\n  \\n\\n\\n\\n\\n\\n    Myscale\\n  \\n\\n\\n\\n\\n\\n    Notion\\n  \\n\\n\\n\\n\\n\\n    Nougat ocr\\n  \\n\\n\\n\\n\\n\\n    Obsidian\\n  \\n\\n\\n\\n\\n\\n    Openalex\\n  \\n\\n\\n\\n\\n\\n    Openapi\\n  \\n\\n\\n\\n\\n\\n    Opendal\\n  \\n\\n\\n\\n\\n\\n    Opensearch\\n  \\n\\n\\n\\n\\n\\n    Pandas ai\\n  \\n\\n\\n\\n\\n\\n    Papers\\n  \\n\\n\\n\\n\\n\\n    Patentsview\\n  \\n\\n\\n\\n\\n\\n    Pathway\\n  \\n\\n\\n\\n\\n\\n    Pdb\\n  \\n\\n\\n\\n\\n\\n    Pdf table\\n  \\n\\n\\n\\n\\n\\n    Pebblo\\n  \\n\\n\\n\\n\\n\\n    None\\n  \\n\\n\\n\\n\\n\\n    Preprocess\\n  \\n\\n\\n\\n\\n\\n    Psychic\\n  \\n\\n\\n\\n\\n\\n    Qdrant\\n  \\n\\n\\n\\n\\n\\n    Rayyan\\n  \\n\\n\\n\\n\\n\\n    Readme\\n  \\n\\n\\n\\n\\n\\n    Readwise\\n  \\n\\n\\n\\n\\n\\n    Reddit\\n  \\n\\n\\n\\n\\n\\n    Remote\\n  \\n\\n\\n\\n\\n\\n    Remote depth\\n  \\n\\n\\n\\n\\n\\n    S3\\n  \\n\\n\\n\\n\\n\\n    Sec filings\\n  \\n\\n\\n\\n\\n\\n    Semanticscholar\\n  \\n\\n\\n\\n\\n\\n    Simple directory reader\\n  \\n\\n\\n\\n\\n\\n    Singlestore\\n  \\n\\n\\n\\n\\n\\n    Slack\\n  \\n\\n\\n\\n\\n\\n    Smart pdf loader\\n  \\n\\n\\n\\n\\n\\n    Snowflake\\n  \\n\\n\\n\\n\\n\\n    Snscrape twitter\\n  \\n\\n\\n\\n\\n\\n    Spotify\\n  \\n\\n\\n\\n\\n\\n    Stackoverflow\\n  \\n\\n\\n\\n\\n\\n    Steamship\\n  \\n\\n\\n\\n\\n\\n    String iterable\\n  \\n\\n\\n\\n\\n\\n    Stripe docs\\n  \\n\\n\\n\\n\\n\\n    Telegram\\n  \\n\\n\\n\\n\\n\\n    Trello\\n  \\n\\n\\n\\n\\n\\n    Twitter\\n  \\n\\n\\n\\n\\n\\n    Txtai\\n  \\n\\n\\n\\n\\n\\n    Weather\\n  \\n\\n\\n\\n\\n\\n    Weaviate\\n  \\n\\n\\n\\n\\n\\n    Web\\n  \\n\\n\\n\\n\\n\\n    Whatsapp\\n  \\n\\n\\n\\n\\n\\n    Wikipedia\\n  \\n\\n\\n\\n\\n\\n    Wordlift\\n  \\n\\n\\n\\n\\n\\n    Wordpress\\n  \\n\\n\\n\\n\\n\\n    Youtube metadata\\n  \\n\\n\\n\\n\\n\\n    Youtube transcript\\n  \\n\\n\\n\\n\\n\\n    Zendesk\\n  \\n\\n\\n\\n\\n\\n    Zep\\n  \\n\\n\\n\\n\\n\\n    Zulip\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Response Synthesizers\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Response Synthesizers\\n          \\n\\n\\n\\n\\n    Accumulate\\n  \\n\\n\\n\\n\\n\\n    Compact accumulate\\n  \\n\\n\\n\\n\\n\\n    Compact and refine\\n  \\n\\n\\n\\n\\n\\n    Generation\\n  \\n\\n\\n\\n\\n\\n    Google\\n  \\n\\n\\n\\n\\n\\n    Refine\\n  \\n\\n\\n\\n\\n\\n    Simple summarize\\n  \\n\\n\\n\\n\\n\\n    Tree summarize\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Retrievers\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Retrievers\\n          \\n\\n\\n\\n\\n    Auto merging\\n  \\n\\n\\n\\n\\n\\n    Bedrock\\n  \\n\\n\\n\\n\\n\\n    Bm25\\n  \\n\\n\\n\\n\\n\\n    Keyword\\n  \\n\\n\\n\\n\\n\\n    Knowledge graph\\n  \\n\\n\\n\\n\\n\\n    Mongodb atlas bm25 retriever\\n  \\n\\n\\n\\n\\n\\n    Pathway\\n  \\n\\n\\n\\n\\n\\n    Query fusion\\n  \\n\\n\\n\\n\\n\\n    Recursive\\n  \\n\\n\\n\\n\\n\\n    Router\\n  \\n\\n\\n\\n\\n\\n    Sql\\n  \\n\\n\\n\\n\\n\\n    Summary\\n  \\n\\n\\n\\n\\n\\n    Transform\\n  \\n\\n\\n\\n\\n\\n    Tree\\n  \\n\\n\\n\\n\\n\\n    Vector\\n  \\n\\n\\n\\n\\n\\n    Videodb\\n  \\n\\n\\n\\n\\n\\n    You\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Schema\\n  \\n\\n\\n\\n\\n\\n            Schema\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n    Storage\\n  \\n\\n\\n\\n\\n\\n            Storage\\n          \\n\\n\\n\\n\\n\\n\\n    Chat Store\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Chat Store\\n          \\n\\n\\n\\n\\n    Azure\\n  \\n\\n\\n\\n\\n\\n    Redis\\n  \\n\\n\\n\\n\\n\\n    Simple\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Docstore\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Docstore\\n          \\n\\n\\n\\n\\n    Azure\\n  \\n\\n\\n\\n\\n\\n    Dynamodb\\n  \\n\\n\\n\\n\\n\\n    Elasticsearch\\n  \\n\\n\\n\\n\\n\\n    Firestore\\n  \\n\\n\\n\\n\\n\\n    Mongodb\\n  \\n\\n\\n\\n\\n\\n    Postgres\\n  \\n\\n\\n\\n\\n\\n    Redis\\n  \\n\\n\\n\\n\\n\\n    Simple\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Graph Stores\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Graph Stores\\n          \\n\\n\\n\\n\\n    Falkordb\\n  \\n\\n\\n\\n\\n\\n    Kuzu\\n  \\n\\n\\n\\n\\n\\n    Nebula\\n  \\n\\n\\n\\n\\n\\n    Neo4j\\n  \\n\\n\\n\\n\\n\\n    Neptune\\n  \\n\\n\\n\\n\\n\\n    Simple\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Index Store\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Index Store\\n          \\n\\n\\n\\n\\n    Azure\\n  \\n\\n\\n\\n\\n\\n    Dynamodb\\n  \\n\\n\\n\\n\\n\\n    Elasticsearch\\n  \\n\\n\\n\\n\\n\\n    Firestore\\n  \\n\\n\\n\\n\\n\\n    Mongodb\\n  \\n\\n\\n\\n\\n\\n    Postgres\\n  \\n\\n\\n\\n\\n\\n    Redis\\n  \\n\\n\\n\\n\\n\\n    Simple\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Kvstore\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Kvstore\\n          \\n\\n\\n\\n\\n    Azure\\n  \\n\\n\\n\\n\\n\\n    Dynamodb\\n  \\n\\n\\n\\n\\n\\n    Elasticsearch\\n  \\n\\n\\n\\n\\n\\n    Firestore\\n  \\n\\n\\n\\n\\n\\n    Mongodb\\n  \\n\\n\\n\\n\\n\\n    Postgres\\n  \\n\\n\\n\\n\\n\\n    Redis\\n  \\n\\n\\n\\n\\n\\n    S3\\n  \\n\\n\\n\\n\\n\\n    Simple\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Storage\\n  \\n\\n\\n\\n\\n\\n            Storage\\n          \\n\\n\\n\\n\\n    Storage context\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Vector Store\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Vector Store\\n          \\n\\n\\n\\n\\n    Alibabacloud opensearch\\n  \\n\\n\\n\\n\\n\\n    Analyticdb\\n  \\n\\n\\n\\n\\n\\n    Astra db\\n  \\n\\n\\n\\n\\n\\n    Awadb\\n  \\n\\n\\n\\n\\n\\n    Awsdocdb\\n  \\n\\n\\n\\n\\n\\n    Azureaisearch\\n  \\n\\n\\n\\n\\n\\n    Azurecosmosmongo\\n  \\n\\n\\n\\n\\n\\n    Bagel\\n  \\n\\n\\n\\n\\n\\n    Baiduvectordb\\n  \\n\\n\\n\\n\\n\\n    Cassandra\\n  \\n\\n\\n\\n\\n\\n    Chatgpt plugin\\n  \\n\\n\\n\\n\\n\\n    Chroma\\n  \\n\\n\\n\\n\\n\\n    Clickhouse\\n  \\n\\n\\n\\n\\n\\n    Couchbase\\n  \\n\\n\\n\\n\\n\\n    Dashvector\\n  \\n\\n\\n\\n\\n\\n    Databricks\\n  \\n\\n\\n\\n\\n\\n    Deeplake\\n  \\n\\n\\n\\n\\n\\n    Docarray\\n  \\n\\n\\n\\n\\n\\n    Duckdb\\n  \\n\\n\\n\\n\\n\\n    Dynamodb\\n  \\n\\n\\n\\n\\n\\n    Elasticsearch\\n  \\n\\n\\n\\n\\n\\n    Epsilla\\n  \\n\\n\\n\\n\\n\\n    Faiss\\n  \\n\\n\\n\\n\\n\\n    Firestore\\n  \\n\\n\\n\\n\\n\\n    Google\\n  \\n\\n\\n\\n\\n\\n    Jaguar\\n  \\n\\n\\n\\n\\n\\n    Kdbai\\n  \\n\\n\\n\\n\\n\\n    Lancedb\\n  \\n\\n\\n\\n\\n\\n    Lantern\\n  \\n\\n\\n\\n\\n\\n    Metal\\n  \\n\\n\\n\\n\\n\\n    Milvus\\n  \\n\\n\\n\\n\\n\\n    Mongodb\\n  \\n\\n\\n\\n\\n\\n    Myscale\\n  \\n\\n\\n\\n\\n\\n    Neo4jvector\\n  \\n\\n\\n\\n\\n\\n    Neptune\\n  \\n\\n\\n\\n\\n\\n    Opensearch\\n  \\n\\n\\n\\n\\n\\n    Pgvecto rs\\n  \\n\\n\\n\\n\\n\\n    Pinecone\\n  \\n\\n\\n\\n\\n\\n    Postgres\\n  \\n\\n\\n\\n\\n\\n    Qdrant\\n  \\n\\n\\n\\n\\n\\n    Redis\\n  \\n\\n\\n\\n\\n\\n    Relyt\\n  \\n\\n\\n\\n\\n\\n    Rocksetdb\\n  \\n\\n\\n\\n\\n\\n    Simple\\n  \\n\\n\\n\\n\\n\\n    Singlestoredb\\n  \\n\\n\\n\\n\\n\\n    Supabase\\n  \\n\\n\\n\\n\\n\\n    Tair\\n  \\n\\n\\n\\n\\n\\n    Tencentvectordb\\n  \\n\\n\\n\\n\\n\\n    Tidbvector\\n  \\n\\n\\n\\n\\n\\n    Timescalevector\\n  \\n\\n\\n\\n\\n\\n    Txtai\\n  \\n\\n\\n\\n\\n\\n    Typesense\\n  \\n\\n\\n\\n\\n\\n    Upstash\\n  \\n\\n\\n\\n\\n\\n    Vearch\\n  \\n\\n\\n\\n\\n\\n    Vertexaivectorsearch\\n  \\n\\n\\n\\n\\n\\n    Vespa\\n  \\n\\n\\n\\n\\n\\n    Weaviate\\n  \\n\\n\\n\\n\\n\\n    Wordlift\\n  \\n\\n\\n\\n\\n\\n    Zep\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Tools\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Tools\\n          \\n\\n\\n\\n\\n    Arxiv\\n  \\n\\n\\n\\n\\n\\n    Azure code interpreter\\n  \\n\\n\\n\\n\\n\\n    Azure cv\\n  \\n\\n\\n\\n\\n\\n    Azure speech\\n  \\n\\n\\n\\n\\n\\n    Azure translate\\n  \\n\\n\\n\\n\\n\\n    Bing search\\n  \\n\\n\\n\\n\\n\\n    Brave search\\n  \\n\\n\\n\\n\\n\\n    Cassandra\\n  \\n\\n\\n\\n\\n\\n    Chatgpt plugin\\n  \\n\\n\\n\\n\\n\\n    Code interpreter\\n  \\n\\n\\n\\n\\n\\n    Cogniswitch\\n  \\n\\n\\n\\n\\n\\n    Database\\n  \\n\\n\\n\\n\\n\\n    Duckduckgo\\n  \\n\\n\\n\\n\\n\\n    Exa\\n  \\n\\n\\n\\n\\n\\n    Finance\\n  \\n\\n\\n\\n\\n\\n    Function\\n  \\n\\n\\n\\n\\n\\n    Google\\n  \\n\\n\\n\\n\\n\\n    Graphql\\n  \\n\\n\\n\\n\\n\\n    Ionic shopping\\n  \\n\\n\\n\\n\\n\\n    Load and search\\n  \\n\\n\\n\\n\\n\\n    Metaphor\\n  \\n\\n\\n\\n\\n\\n    Multion\\n  \\n\\n\\n\\n\\n\\n    Neo4j\\n  \\n\\n\\n\\n\\n\\n    Notion\\n  \\n\\n\\n\\n\\n\\n    Ondemand loader\\n  \\n\\n\\n\\n\\n\\n    Openai\\n  \\n\\n\\n\\n\\n\\n    Openapi\\n  \\n\\n\\n\\n\\n\\n    Passio nutrition ai\\n  \\n\\n\\n\\n\\n\\n    Playgrounds\\n  \\n\\n\\n\\n\\n\\n    Python file\\n  \\n\\n\\n\\n\\n\\n    Query engine\\n  \\n\\n\\n\\n\\n\\n    Query plan\\n  \\n\\n\\n\\n\\n\\n    Requests\\n  \\n\\n\\n\\n\\n\\n    Retriever\\n  \\n\\n\\n\\n\\n\\n    Salesforce\\n  \\n\\n\\n\\n\\n\\n    Shopify\\n  \\n\\n\\n\\n\\n\\n    Slack\\n  \\n\\n\\n\\n\\n\\n    Tavily research\\n  \\n\\n\\n\\n\\n\\n    Text to image\\n  \\n\\n\\n\\n\\n\\n    Tool spec\\n  \\n\\n\\n\\n\\n\\n    Vector db\\n  \\n\\n\\n\\n\\n\\n    Waii\\n  \\n\\n\\n\\n\\n\\n    Weather\\n  \\n\\n\\n\\n\\n\\n    Wikipedia\\n  \\n\\n\\n\\n\\n\\n    Wolfram alpha\\n  \\n\\n\\n\\n\\n\\n    Yahoo finance\\n  \\n\\n\\n\\n\\n\\n    Yelp\\n  \\n\\n\\n\\n\\n\\n    Zapier\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Open-Source Community\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Open-Source Community\\n          \\n\\n\\n\\n\\n    Integrations\\n  \\n\\n\\n\\n\\n\\n    Full Stack Projects\\n  \\n\\n\\n\\n\\n\\n\\n    Community FAQ\\n  \\n\\n\\n\\n\\n\\n            Community FAQ\\n          \\n\\n\\n\\n\\n    Chat Engines\\n  \\n\\n\\n\\n\\n\\n    Documents and Nodes\\n  \\n\\n\\n\\n\\n\\n    Embeddings\\n  \\n\\n\\n\\n\\n\\n    Large Language Models\\n  \\n\\n\\n\\n\\n\\n    Query Engines\\n  \\n\\n\\n\\n\\n\\n    Vector Database\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Contributing\\n  \\n\\n\\n\\n\\n\\n            Contributing\\n          \\n\\n\\n\\n\\n    Code\\n  \\n\\n\\n\\n\\n\\n    Docs\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Changelog\\n  \\n\\n\\n\\n\\n\\n    Presentations\\n  \\n\\n\\n\\n\\n\\n    Upgrading to v0.10.x\\n  \\n\\n\\n\\n\\n\\n    Deprecated Terms\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LlamaCloud\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            LlamaCloud\\n          \\n\\n\\n\\n\\n    LlamaParse\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Installation and Setup\\n    \\n\\n\\n\\n\\n\\n\\n      Setup LLM using Groq\\n    \\n\\n\\n\\n\\n\\n      Setup Embedding Model\\n    \\n\\n\\n\\n\\n\\n      Define Global Settings Configuration\\n    \\n\\n\\n\\n\\n\\n      Download Data\\n    \\n\\n\\n\\n\\n\\n      Load Data\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      1. Basic Completion and Chat\\n    \\n\\n\\n\\n\\n\\n\\n      Call complete with a prompt\\n    \\n\\n\\n\\n\\n\\n      Call chat with a list of messages\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      2. Basic RAG (Vector Search, Summarization)\\n    \\n\\n\\n\\n\\n\\n\\n      Basic RAG (Vector Search)\\n    \\n\\n\\n\\n\\n\\n      Basic RAG (Summarization)\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      3. Advanced RAG (Routing)\\n    \\n\\n\\n\\n\\n\\n\\n      Build a Router that can choose whether to do vector search or summarization\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      4. Text-to-SQL\\n    \\n\\n\\n\\n\\n\\n      5. Structured Data Extraction\\n    \\n\\n\\n\\n\\n\\n      6. Adding Chat History to RAG (Chat Engine)\\n    \\n\\n\\n\\n\\n\\n      7. Agents\\n    \\n\\n\\n\\n\\n\\n\\n      Agents And Tools\\n    \\n\\n\\n\\n\\n\\n      Define Tools\\n    \\n\\n\\n\\n\\n\\n      ReAct Agent\\n    \\n\\n\\n\\n\\n\\n      Querying\\n    \\n\\n\\n\\n\\n\\n      ReAct Agent With RAG QueryEngine Tools\\n    \\n\\n\\n\\n\\n\\n      Create ReAct Agent using RAG QueryEngine Tools\\n    \\n\\n\\n\\n\\n\\n      Querying\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLlama3 Cookbook with Groq¬∂\\nMeta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks.\\nIn this notebook, we demonstrate how to use Llama3 with LlamaIndex for a comprehensive set of use cases.\\n\\nBasic completion / chat\\nBasic RAG (Vector Search, Summarization)\\nAdvanced RAG (Routing)\\nText-to-SQL\\nStructured Data Extraction\\nChat Engine + Memory\\nAgents\\n\\nWe use Llama3-8B and Llama3-70B through Groq.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInstallation and Setup¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\n!pip install llama-index\\n!pip install llama-index-llms-groq\\n!pip install llama-index-embeddings-huggingface\\n!pip install llama-parse\\n\\n!pip install llama-index\\n!pip install llama-index-llms-groq\\n!pip install llama-index-embeddings-huggingface\\n!pip install llama-parse\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSetup LLM using Groq¬∂To use Groq, you need to make sure that GROQ_API_KEY is specified as an environment variable.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nimport os\\n\\nos.environ[\"GROQ_API_KEY\"] = \"<GROQ_API_KEY>\"\\n\\nimport os\\n\\nos.environ[\"GROQ_API_KEY\"] = \"\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_index.llms.groq import Groq\\n\\nllm = Groq(model=\"llama3-8b-8192\")\\nllm_70b = Groq(model=\"llama3-70b-8192\")\\n\\nfrom llama_index.llms.groq import Groq\\n\\nllm = Groq(model=\"llama3-8b-8192\")\\nllm_70b = Groq(model=\"llama3-70b-8192\")\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSetup Embedding Model¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\\n\\nembed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\\n\\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\\n\\nembed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDefine Global Settings Configuration¬∂In LlamaIndex, you can define global settings so you don\\'t have to pass the LLM / embedding model objects everywhere.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_index.core import Settings\\n\\nSettings.llm = llm\\nSettings.embed_model = embed_model\\n\\nfrom llama_index.core import Settings\\n\\nSettings.llm = llm\\nSettings.embed_model = embed_model\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDownload Data¬∂Here you\\'ll download data that\\'s used in section 2 and onwards.\\nWe\\'ll download some articles on Kendrick, Drake, and their beef (as of May 2024).\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\n!mkdir data\\n!wget \"https://www.dropbox.com/scl/fi/t1soxfjdp0v44an6sdymd/drake_kendrick_beef.pdf?rlkey=u9546ymb7fj8lk2v64r6p5r5k&st=wjzzrgil&dl=1\" -O data/drake_kendrick_beef.pdf\\n!wget \"https://www.dropbox.com/scl/fi/nts3n64s6kymner2jppd6/drake.pdf?rlkey=hksirpqwzlzqoejn55zemk6ld&st=mohyfyh4&dl=1\" -O data/drake.pdf\\n!wget \"https://www.dropbox.com/scl/fi/8ax2vnoebhmy44bes2n1d/kendrick.pdf?rlkey=fhxvn94t5amdqcv9vshifd3hj&st=dxdtytn6&dl=1\" -O data/kendrick.pdf\\n\\n!mkdir data\\n!wget \"https://www.dropbox.com/scl/fi/t1soxfjdp0v44an6sdymd/drake_kendrick_beef.pdf?rlkey=u9546ymb7fj8lk2v64r6p5r5k&st=wjzzrgil&dl=1\" -O data/drake_kendrick_beef.pdf\\n!wget \"https://www.dropbox.com/scl/fi/nts3n64s6kymner2jppd6/drake.pdf?rlkey=hksirpqwzlzqoejn55zemk6ld&st=mohyfyh4&dl=1\" -O data/drake.pdf\\n!wget \"https://www.dropbox.com/scl/fi/8ax2vnoebhmy44bes2n1d/kendrick.pdf?rlkey=fhxvn94t5amdqcv9vshifd3hj&st=dxdtytn6&dl=1\" -O data/kendrick.pdf\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoad Data¬∂We load data using LlamaParse by default, but you can also choose to opt for our free pypdf reader (in SimpleDirectoryReader by default) if you don\\'t have an account!\\n\\nLlamaParse: Signup for an account here: cloud.llamaindex.ai. You get 1k free pages a day, and paid plan is 7k free pages + 0.3c per additional page. LlamaParse is a good option if you want to parse complex documents, like PDFs with charts, tables, and more.\\n\\nDefault PDF Parser (In SimpleDirectoryReader). If you don\\'t want to signup for an account / use a PDF service, just use the default PyPDF reader bundled in our file loader. It\\'s a good choice for getting started!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_parse import LlamaParse\\n\\ndocs_kendrick = LlamaParse(result_type=\"text\").load_data(\"./data/kendrick.pdf\")\\ndocs_drake = LlamaParse(result_type=\"text\").load_data(\"./data/drake.pdf\")\\ndocs_both = LlamaParse(result_type=\"text\").load_data(\\n    \"./data/drake_kendrick_beef.pdf\"\\n)\\n\\n\\n# from llama_index.core import SimpleDirectoryReader\\n\\n# docs_kendrick = SimpleDirectoryReader(input_files=[\"data/kendrick.pdf\"]).load_data()\\n# docs_drake = SimpleDirectoryReader(input_files=[\"data/drake.pdf\"]).load_data()\\n# docs_both = SimpleDirectoryReader(input_files=[\"data/drake_kendrick_beef.pdf\"]).load_data()\\n\\nfrom llama_parse import LlamaParse\\n\\ndocs_kendrick = LlamaParse(result_type=\"text\").load_data(\"./data/kendrick.pdf\")\\ndocs_drake = LlamaParse(result_type=\"text\").load_data(\"./data/drake.pdf\")\\ndocs_both = LlamaParse(result_type=\"text\").load_data(\\n    \"./data/drake_kendrick_beef.pdf\"\\n)\\n\\n\\n# from llama_index.core import SimpleDirectoryReader\\n\\n# docs_kendrick = SimpleDirectoryReader(input_files=[\"data/kendrick.pdf\"]).load_data()\\n# docs_drake = SimpleDirectoryReader(input_files=[\"data/drake.pdf\"]).load_data()\\n# docs_both = SimpleDirectoryReader(input_files=[\"data/drake_kendrick_beef.pdf\"]).load_data()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nStarted parsing the file under job_id 391f5fe8-aed3-46a3-af7d-18341b1b20d7\\nStarted parsing the file under job_id 08c335d5-417b-4249-b53d-a7a9b65293a8\\nStarted parsing the file under job_id e3a91a73-5db0-4df0-9590-c9393cb048cf\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1. Basic Completion and Chat¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCall complete with a prompt¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nresponse = llm.complete(\"do you like drake or kendrick better?\")\\n\\nprint(response)\\n\\nresponse = llm.complete(\"do you like drake or kendrick better?\")\\n\\nprint(response)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nI\\'m just an AI, I don\\'t have personal preferences or opinions, nor do I have the capacity to enjoy or dislike music. I can provide information and insights about different artists and their work, but I don\\'t have personal feelings or biases.\\n\\nHowever, I can tell you that both Drake and Kendrick Lamar are highly acclaimed and influential artists in the music industry. They have both received widespread critical acclaim and have won numerous awards for their work.\\n\\nDrake is known for his introspective and emotive lyrics, as well as his ability to blend different genres such as hip-hop, R&B, and pop. He has been praised for his storytelling ability and his ability to connect with his audience.\\n\\nKendrick Lamar, on the other hand, is known for his socially conscious lyrics and his ability to tackle complex issues such as racism, inequality, and social justice. He has been praised for his lyrical depth and his ability to blend different genres such as hip-hop, jazz, and funk.\\n\\nUltimately, whether you prefer Drake or Kendrick Lamar depends on your personal taste in music and what you value in an artist.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nstream_response = llm.stream_complete(\\n    \"you\\'re a drake fan. tell me why you like drake more than kendrick\"\\n)\\n\\nfor t in stream_response:\\n    print(t.delta, end=\"\")\\n\\nstream_response = llm.stream_complete(\\n    \"you\\'re a drake fan. tell me why you like drake more than kendrick\"\\n)\\n\\nfor t in stream_response:\\n    print(t.delta, end=\"\")\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMan, I\\'m a die-hard Drake fan, and I gotta say, I love the 6 God for many reasons. Now, I know some people might say Kendrick is the king of hip-hop, and I respect that, but for me, Drake\\'s got something special. Here\\'s why:\\n\\n1. **Relatability**: Drake\\'s lyrics are like a diary entry. He\\'s got this ability to tap into the emotions and struggles of everyday people. His songs are like a reflection of our own experiences, you know? He\\'s not just rapping about gangsta life or material possessions; he\\'s talking about the real stuff, like relationships, fame, and the struggles of growing up. That\\'s what makes his music so relatable and authentic.\\n\\n2. **Vocal delivery**: Drake\\'s got this smooth, melodic flow that\\'s unmatched. His vocals are like butter ‚Äì they just glide over the beat. He\\'s got this effortless swag that makes his songs feel like a warm hug on a cold day. Kendrick\\'s got a great flow too, but Drake\\'s got this unique, laid-back vibe that\\'s hard to replicate.\\n\\n3. **Storytelling**: Drake\\'s a master storyteller. He\\'s got this ability to paint vivid pictures with his words, taking you on a journey through his life experiences. His songs are like mini-movies, with characters, settings, and plot twists. Kendrick\\'s got great storytelling skills too, but Drake\\'s got this extra something that makes his stories feel more intimate and personal.\\n\\n4. **Production**: Drake\\'s got an ear for beats that\\'s unmatched. He\\'s always pushing the boundaries of what a hip-hop beat can be. From the atmospheric soundscapes of \"Take Care\" to the trap-infused bangers of \"Scorpion,\" Drake\\'s always experimenting and innovating. Kendrick\\'s got great production too, but Drake\\'s got this versatility that\\'s hard to match.\\n\\n5. **Emotional depth**: Drake\\'s music is like a therapy session. He\\'s not afraid to get vulnerable and open up about his emotions. He\\'s got this ability to tap into the human experience and share his own struggles and triumphs. Kendrick\\'s got great emotional depth too, but Drake\\'s got this extra layer of vulnerability that makes his music feel more honest and authentic.\\n\\nSo, there you have it ‚Äì that\\'s why I\\'m a Drake fan through and through. He\\'s got this unique blend of relatability, vocal delivery, storytelling, production, and emotional depth that sets him apart from the rest.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCall chat with a list of messages¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_index.core.llms import ChatMessage\\n\\nmessages = [\\n    ChatMessage(role=\"system\", content=\"You are Kendrick.\"),\\n    ChatMessage(role=\"user\", content=\"Write a verse.\"),\\n]\\nresponse = llm.chat(messages)\\n\\nfrom llama_index.core.llms import ChatMessage\\n\\nmessages = [\\n    ChatMessage(role=\"system\", content=\"You are Kendrick.\"),\\n    ChatMessage(role=\"user\", content=\"Write a verse.\"),\\n]\\nresponse = llm.chat(messages)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nprint(response)\\n\\nprint(response)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nassistant: \"I\\'m the king of the game, no debate\\nMy rhymes so tight, they\\'re like a weight\\nI\\'m the voice of the streets, the people\\'s champ\\nMy flow\\'s on fire, leaving the haters in the slam\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2. Basic RAG (Vector Search, Summarization)¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBasic RAG (Vector Search)¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_index.core import VectorStoreIndex\\n\\nindex = VectorStoreIndex.from_documents(docs_both)\\nquery_engine = index.as_query_engine(similarity_top_k=3)\\n\\nfrom llama_index.core import VectorStoreIndex\\n\\nindex = VectorStoreIndex.from_documents(docs_both)\\nquery_engine = index.as_query_engine(similarity_top_k=3)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nresponse = query_engine.query(\"Tell me about family matters\")\\n\\nresponse = query_engine.query(\"Tell me about family matters\")\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nprint(str(response))\\n\\nprint(str(response))\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe concept of \"Family Matters\" is a central theme in the ongoing beef between Drake and Kendrick Lamar. It refers to a seven-and-a-half-minute diss track released by Drake in response to Kendrick\\'s diss track \"Family Matters.\" The track is a scathing attack on Kendrick, with Drake addressing various allegations and accusations made by Kendrick. The track is notable for its dark and sinister tone, with Drake delivering a series of personal attacks on Kendrick and his family. The track also features Drake addressing his own family, including his son Adonis and his parents, Dennis and Sandi Graham.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBasic RAG (Summarization)¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_index.core import SummaryIndex\\n\\nsummary_index = SummaryIndex.from_documents(docs_both)\\nsummary_engine = summary_index.as_query_engine()\\n\\nfrom llama_index.core import SummaryIndex\\n\\nsummary_index = SummaryIndex.from_documents(docs_both)\\nsummary_engine = summary_index.as_query_engine()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nresponse = summary_engine.query(\\n    \"Given your assessment of this article, who won the beef?\"\\n)\\n\\nresponse = summary_engine.query(\\n    \"Given your assessment of this article, who won the beef?\"\\n)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nprint(str(response))\\n\\nprint(str(response))\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIt\\'s difficult to declare a clear winner in this beef, as both Kendrick Lamar and Drake have delivered scathing diss tracks, and the beef has been marked by a series of intense exchanges.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3. Advanced RAG (Routing)¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBuild a Router that can choose whether to do vector search or summarization¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\\n\\nvector_tool = QueryEngineTool(\\n    index.as_query_engine(),\\n    metadata=ToolMetadata(\\n        name=\"vector_search\",\\n        description=\"Useful for searching for specific facts.\",\\n    ),\\n)\\n\\nsummary_tool = QueryEngineTool(\\n    index.as_query_engine(response_mode=\"tree_summarize\"),\\n    metadata=ToolMetadata(\\n        name=\"summary\",\\n        description=\"Useful for summarizing an entire document.\",\\n    ),\\n)\\n\\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\\n\\nvector_tool = QueryEngineTool(\\n    index.as_query_engine(),\\n    metadata=ToolMetadata(\\n        name=\"vector_search\",\\n        description=\"Useful for searching for specific facts.\",\\n    ),\\n)\\n\\nsummary_tool = QueryEngineTool(\\n    index.as_query_engine(response_mode=\"tree_summarize\"),\\n    metadata=ToolMetadata(\\n        name=\"summary\",\\n        description=\"Useful for summarizing an entire document.\",\\n    ),\\n)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_index.core.query_engine import RouterQueryEngine\\n\\nquery_engine = RouterQueryEngine.from_defaults(\\n    [vector_tool, summary_tool], select_multi=False, verbose=True, llm=llm_70b\\n)\\n\\nresponse = query_engine.query(\\n    \"Tell me about the song meet the grahams - why is it significant\"\\n)\\n\\nfrom llama_index.core.query_engine import RouterQueryEngine\\n\\nquery_engine = RouterQueryEngine.from_defaults(\\n    [vector_tool, summary_tool], select_multi=False, verbose=True, llm=llm_70b\\n)\\n\\nresponse = query_engine.query(\\n    \"Tell me about the song meet the grahams - why is it significant\"\\n)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSelecting query engine 0: The question asks for specific facts about the song \\'Meet the Grahams\\', so a search for specific facts is required..\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nprint(response)\\n\\nprint(response)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe \"Meet the Grahams\" artwork is significant because it\\'s the full picture that Kendrick Lamar teased earlier on \"6.16 in LA.\" It shows a pair of Maybach gloves, a shirt, receipts, and prescription bottles, including one for Ozempic prescribed to Drake.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n4. Text-to-SQL¬∂Here, we download and use a sample SQLite database with 11 tables, with various info about music, playlists, and customers. We will limit to a select few tables for this test.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\n!wget \"https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip\" -O \"./data/chinook.zip\"\\n!unzip \"./data/chinook.zip\"\\n\\n!wget \"https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip\" -O \"./data/chinook.zip\"\\n!unzip \"./data/chinook.zip\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n--2024-05-10 23:40:37--  https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip\\nResolving www.sqlitetutorial.net (www.sqlitetutorial.net)... 2606:4700:3037::6815:1e8d, 2606:4700:3037::ac43:acfa, 104.21.30.141, ...\\nConnecting to www.sqlitetutorial.net (www.sqlitetutorial.net)|2606:4700:3037::6815:1e8d|:443... connected.\\nHTTP request sent, awaiting response... 200 OK\\nLength: 305596 (298K) [application/zip]\\nSaving to: ‚Äò./data/chinook.zip‚Äô\\n\\n./data/chinook.zip  100%[===================>] 298.43K  --.-KB/s    in 0.02s   \\n\\n2024-05-10 23:40:37 (13.9 MB/s) - ‚Äò./data/chinook.zip‚Äô saved [305596/305596]\\n\\n\\n\\n\\n\\n\\n\\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\\nTo disable this warning, you can either:\\n\\t- Avoid using `tokenizers` before the fork if possible\\n\\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\\n\\n\\n\\n\\n\\n\\nArchive:  ./data/chinook.zip\\n  inflating: chinook.db              \\n\\n\\n\\n\\n\\n\\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\\nTo disable this warning, you can either:\\n\\t- Avoid using `tokenizers` before the fork if possible\\n\\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom sqlalchemy import (\\n    create_engine,\\n    MetaData,\\n    Table,\\n    Column,\\n    String,\\n    Integer,\\n    select,\\n    column,\\n)\\n\\nengine = create_engine(\"sqlite:///chinook.db\")\\n\\nfrom sqlalchemy import (\\n    create_engine,\\n    MetaData,\\n    Table,\\n    Column,\\n    String,\\n    Integer,\\n    select,\\n    column,\\n)\\n\\nengine = create_engine(\"sqlite:///chinook.db\")\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_index.core import SQLDatabase\\n\\nsql_database = SQLDatabase(engine)\\n\\nfrom llama_index.core import SQLDatabase\\n\\nsql_database = SQLDatabase(engine)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_index.core.indices.struct_store import NLSQLTableQueryEngine\\n\\nquery_engine = NLSQLTableQueryEngine(\\n    sql_database=sql_database,\\n    tables=[\"albums\", \"tracks\", \"artists\"],\\n    llm=llm_70b,\\n)\\n\\nfrom llama_index.core.indices.struct_store import NLSQLTableQueryEngine\\n\\nquery_engine = NLSQLTableQueryEngine(\\n    sql_database=sql_database,\\n    tables=[\"albums\", \"tracks\", \"artists\"],\\n    llm=llm_70b,\\n)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nresponse = query_engine.query(\"What are some albums?\")\\n\\nprint(response)\\n\\nresponse = query_engine.query(\"What are some albums?\")\\n\\nprint(response)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHere are some albums: For Those About To Rock We Salute You, Balls to the Wall, Restless and Wild, Let There Be Rock, Big Ones, Jagged Little Pill, Facelift, Warner 25 Anos, Plays Metallica By Four Cellos, and Audioslave.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nresponse = query_engine.query(\"What are some artists? Limit it to 5.\")\\n\\nprint(response)\\n\\nresponse = query_engine.query(\"What are some artists? Limit it to 5.\")\\n\\nprint(response)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHere are 5 artists: AC/DC, Accept, Aerosmith, Alanis Morissette, and Alice In Chains.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThis last query should be a more complex join\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nresponse = query_engine.query(\\n    \"What are some tracks from the artist AC/DC? Limit it to 3\"\\n)\\n\\nprint(response)\\n\\nresponse = query_engine.query(\\n    \"What are some tracks from the artist AC/DC? Limit it to 3\"\\n)\\n\\nprint(response)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHere are three tracks from the legendary Australian rock band AC/DC: \"For Those About To Rock (We Salute You)\", \"Put The Finger On You\", and \"Let\\'s Get It Up\".\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nprint(response.metadata[\"sql_query\"])\\n\\nprint(response.metadata[\"sql_query\"])\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSELECT tracks.Name FROM tracks INNER JOIN albums ON tracks.AlbumId = albums.AlbumId INNER JOIN artists ON albums.ArtistId = artists.ArtistId WHERE artists.Name = \\'AC/DC\\' LIMIT 3;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n5. Structured Data Extraction¬∂An important use case for function calling is extracting structured objects. LlamaIndex provides an intuitive interface for this through structured_predict - simply define the target Pydantic class (can be nested), and given a prompt, we extract out the desired object.\\nNOTE: Since there\\'s no native function calling support with Llama3, the structured extraction is performed by prompting the LLM + output parsing.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_index.llms.groq import Groq\\nfrom llama_index.core.prompts import PromptTemplate\\nfrom pydantic import BaseModel\\n\\n\\nclass Restaurant(BaseModel):\\n    \"\"\"A restaurant with name, city, and cuisine.\"\"\"\\n\\n    name: str\\n    city: str\\n    cuisine: str\\n\\n\\nllm = Groq(model=\"llama3-8b-8192\", pydantic_program_mode=\"llm\")\\nprompt_tmpl = PromptTemplate(\\n    \"Generate a restaurant in a given city {city_name}\"\\n)\\n\\nfrom llama_index.llms.groq import Groq\\nfrom llama_index.core.prompts import PromptTemplate\\nfrom pydantic import BaseModel\\n\\n\\nclass Restaurant(BaseModel):\\n    \"\"\"A restaurant with name, city, and cuisine.\"\"\"\\n\\n    name: str\\n    city: str\\n    cuisine: str\\n\\n\\nllm = Groq(model=\"llama3-8b-8192\", pydantic_program_mode=\"llm\")\\nprompt_tmpl = PromptTemplate(\\n    \"Generate a restaurant in a given city {city_name}\"\\n)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nrestaurant_obj = llm.structured_predict(\\n    Restaurant, prompt_tmpl, city_name=\"Miami\"\\n)\\nprint(restaurant_obj)\\n\\nrestaurant_obj = llm.structured_predict(\\n    Restaurant, prompt_tmpl, city_name=\"Miami\"\\n)\\nprint(restaurant_obj)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nname=\\'Caf√© Havana\\' city=\\'Miami\\' cuisine=\\'Cuban\\'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n6. Adding Chat History to RAG (Chat Engine)¬∂In this section we create a stateful chatbot from a RAG pipeline, with our chat engine abstraction.\\nUnlike a stateless query engine, the chat engine maintains conversation history (through a memory module like buffer memory). It performs retrieval given a condensed question, and feeds the condensed question + context + chat history into the final LLM prompt.\\nRelated resource: https://docs.llamaindex.ai/en/stable/examples/chat_engine/chat_engine_condense_plus_context/\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_index.core.memory import ChatMemoryBuffer\\nfrom llama_index.core.chat_engine import CondensePlusContextChatEngine\\n\\nmemory = ChatMemoryBuffer.from_defaults(token_limit=3900)\\n\\nchat_engine = CondensePlusContextChatEngine.from_defaults(\\n    index.as_retriever(),\\n    memory=memory,\\n    llm=llm,\\n    context_prompt=(\\n        \"You are a chatbot, able to have normal interactions, as well as talk\"\\n        \" about the Kendrick and Drake beef.\"\\n        \"Here are the relevant documents for the context:\\\\n\"\\n        \"{context_str}\"\\n        \"\\\\nInstruction: Use the previous chat history, or the context above, to interact and help the user.\"\\n    ),\\n    verbose=True,\\n)\\n\\nfrom llama_index.core.memory import ChatMemoryBuffer\\nfrom llama_index.core.chat_engine import CondensePlusContextChatEngine\\n\\nmemory = ChatMemoryBuffer.from_defaults(token_limit=3900)\\n\\nchat_engine = CondensePlusContextChatEngine.from_defaults(\\n    index.as_retriever(),\\n    memory=memory,\\n    llm=llm,\\n    context_prompt=(\\n        \"You are a chatbot, able to have normal interactions, as well as talk\"\\n        \" about the Kendrick and Drake beef.\"\\n        \"Here are the relevant documents for the context:\\\\n\"\\n        \"{context_str}\"\\n        \"\\\\nInstruction: Use the previous chat history, or the context above, to interact and help the user.\"\\n    ),\\n    verbose=True,\\n)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nresponse = chat_engine.chat(\\n    \"Tell me about the songs Drake released in the beef.\"\\n)\\nprint(str(response))\\n\\nresponse = chat_engine.chat(\\n    \"Tell me about the songs Drake released in the beef.\"\\n)\\nprint(str(response))\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCondensed question: Tell me about the songs Drake released in the beef.\\nContext: https://www.gq.com/story/the-kendrick-lamar-drake-beef-explained                                        27/34\\n---\\n     5/10/24, 10:08 PM                                        The Kendrick Lamar/Drake Beef, Explained | GQ\\n          May 5: Drake hits back with ‚ÄúThe Heart Part 6‚Äù\\n                     The HEART PART 6   DRAKE\\n\\n\\nTHE HEART PART 6 - DRAKETHE HEART PART 6 - DRAKE\\n          The most productive weekend in the history of rap beef continues, with Drake saying\\n          fuck all to the Sunday Truce and doing exactly what Joe Budden advised: to hit back at\\n          Kendrick\\'s onslaught with a record in the vein of his time-stamp series, straight bars over\\n          a hard beat. Only, Kendrick already beat him to a time-stamp title last week with ‚Äú6:16\\n          in LA,‚Äù so Drake counters by co-opting one of Kendrick\\'s recurring series: ‚ÄúThe Heart.‚Äù\\n          (The last official entry, ‚ÄúThe Heart Part 5,‚Äù heralded Kendrick\\'s Mr. Morale and The Big\\n          Steppers album. Surely you remember the music video, where Kendrick applies deepfake\\n          technology to take on the visages of everyone from Kanye and Nipsey Hussle to OJ.)\\n          Drake even takes a page out of Kendrick\\'s diss manual and applies some classic soul to\\n          the proceedings, countering Kendrick\\'s Teddy Pendergrass and Al Green samples (on\\n          ‚ÄúEuphoria‚Äù and ‚Äú6:16,‚Äù respectively) with an Aretha Franklin sample here.\\n          Aretha sings ‚ÄúLet me see you proooove it,‚Äù setting the tone for Drake\\'s angle here that\\n          Kendrick\\'s been hitting him with baseless accusations. ‚ÄúThe Heart Part 6‚Äù is in full\\n          reaction mode to everything that\\'s transpired over the last three days, including direct\\n         Maial\\n          rebuttals to Kendrick\\'s ‚ÄúNot Like Us;‚Äù it was clearly written in the last 24 hours. DrakeSign up for Manual, our new flagship newsletter\\n                                   Useful advice on style, health, and more, four days a week.\\n          sounds‚Ä¶a little over it all, while nevertheless still promising that shit is about to get\\n          dark. (This is now his second track in a row where he plainly states he\\'d rather be on\\n\\n\\n\\n     https://www.gq.com/story/the-kendrick-lamar-drake-beef-explained                                                              28/34\\n---\\n5/10/24, 10:08 PM                                        The Kendrick Lamar/Drake Beef, Explained | GQ\\n     vacation somewhere than holed up in cold Toronto writing disses.) Drake, buddy,\\n     domestic abuse and pedophilia accusations are in the air‚Äîwe\\'ve been pitch black for the\\n     last few songs already.\\n\\n\\n\\n     You would think Drake would sound a little more celebratory than he does to start the\\n     song, where he takes a victory lap for allegedly going full Sydney Bristow and triple-\\n     crossing Kendrick into leaping on Fake Child Intel. ‚ÄúWe plotted for a week and then we\\n     fed you the information‚Ä¶we thought about giving a fake name or a destination/but you\\n     so thirsty, you not concerned with investigation.‚Äù Who\\'s lying or who was fooled? Only\\n     the Pusha T Investigative Team can solve this.\\n\\n\\n\\n     Drake doesn\\'t dwell there, though, instead moving on to Kendrick\\'s family, doubling\\n     down on the two angles that formed the basis of ‚ÄúFamily Matters‚Äù: that Kendrick has\\n     beaten his partner Whitney in the past, he\\'s estranged from their family, and one of his\\n     two kids is actually fathered by his friend and creative partner Dave Free. To drive this\\n     last point home, ‚ÄúThe Heart Part 6‚Äù artwork is an Instagram screenshot of Dave leaving\\n     heart emojis under, presumably, a picture Whitney posted.\\n\\n\\n\\n     Continuing his through line of using Kendrick\\'s confessional raps on Mr. Morale as\\n     ammo, Drake refers back to ‚ÄúMother I Sober,‚Äù the track where Kendrick unpacks his\\n     mother\\'s sexual abuse and how it informed an incident in his childhood where his\\n     mother was worried he was being abused by a family member even though Kendrick says\\n     he wasn\\'t. Dr. Drake\\'s read: He actually was molseted, and that\\'s why he\\'s so hell-bent\\n     on calling OVO ‚Äúcertified pedophiles.‚Äù\\n\\n5/10/24, 10:08 PMThe Kendrick Lamar/Drake Beef, Explained | GQ\\n          Christopher Polk/Getty Images                                                                 Email address\\n         Maial                       Sign up for Manual, our new flagship newsletter\\n                                     Useful advice on style, health,Cultureand more, four days a week.        SIGN ME UP\\n\\n\\n\\n                     The Kendrick Lamar/Drake Beef, ExplainedNO THANKS\\n\\n\\n\\n     https://www.gq.com/story/the-kendrick-lamar-drake-beef-explained                                                     1/34\\n---\\n5/10/24, 10:08 PM                                         The Kendrick Lamar/Drake Beef, Explained | GQ\\n\\n\\n\\n     Kendrick and Drake diss each other multiple times in one weekend, A.I. shenanigans, shots fired\\n          at and from Future, Metro Boomin, Rick Ross, Weeknd, and more in a new chapter in rap\\n                                                           geopolitics.\\n\\n\\n\\n                                                       By Frazier Tharpe\\n                                                           May 5, 2024\\n\\n\\n\\n     There\\'s the back-to-back effect, and then there\\'s the unrestrained chaos of dropping long\\n     diss tracks, densely loaded with viciously personal power punches, within an hour of each\\n     other. On the first weekend in May, Drake commandeered everyone\\'s Friday night to\\n     turn up the heat in his beef with Kendrick Lamar with a three-part reply and\\n     accompanying music video‚Äîonly for Kendrick to hit right back with what may be one\\n     of the most scathing diss tracks in rap history. This Cold War is firmly and decidedly\\n     thawed all the way out‚Äîand the [Maybach] gloves are off.\\n\\n\\n\\n     To paraphrase prime Jigga-era Jay-Z, the summer just got hotter. Read on for a full\\n     account of 2024\\'s most constantly-evolving rap beef.\\n\\n\\n\\n     Read More\\n\\n\\n\\n     The Drake/Kendrick Lamar Beef Has a Winner. Where Do\\n     We Go From Here?\\n\\n\\n\\n     The low blows thrown during this weekend‚Äôs volley of diss songs have\\n     changed hip-hop‚Äôs rules of engagement forever‚Äîand may have shifted\\n     both Drake and Kendrick‚Äôs legacies in the bargain.\\n     By Lawrence Burney\\n\\n\\n\\n     March 29: Kendrick Lamar declares war, on an album that may be wholly\\n     dedicated to dissing Drake.\\n\\n\\n\\n     Future and Metro Boomin‚Äôs decade-in-the-making new album We Don‚Äôt Trust You was\\n     already one of the most feverishly anticipated rap releases in some time, and on the song\\n    Maial\\n     ‚ÄúLike That,‚Äù Kendrick delivers on that Christmas Eve energy with a guest verse that may\\n                               Sign up for Manual, our new flagship newsletter\\n     as well be a ‚ÄúControl‚Äù sequel. But whereas that name-naming 2013 landmark was\\n                               Useful advice on style, health, and more, four days a week.\\n     ultimately rooted in the spirit of competition, this time the gloves are off and the love is\\n     done.\\n\\n\\n\\nhttps://www.gq.com/story/the-kendrick-lamar-drake-beef-explained                                         2/34\\n---\\n     5/10/24, 10:08 PM                                        The Kendrick Lamar/Drake Beef, Explained | GQ\\n                     Future; Metro Boomin Kendrick Lamar                    Like That (Official Audio)\\n\\n\\nFuture, Metro Boomin, Kendrick Lamar - Like That (OFuture, Metro Boomin, Kendrick Lamar - Like That (Offifficial Audio)cial Audio)\\n          Kendrick sets the tone early, declaring that he‚Äôs ‚Äúchoosing violence‚Äù and it‚Äôs time for an\\n          opponent to ‚Äúprove that he‚Äôs a problem.‚Äù And though no names are officially named, a\\n          reference to Drake‚Äôs song ‚ÄúFirst Person Shooter‚Äù and the album it lives on, For All the\\n          Dogs, means we have to consider this something more than a subliminal. On ‚ÄúFPS‚Äù\\n          Drake brags about taking Michael Jackson‚Äôs mantle for having the most Billboard Hot\\n          100 No. 1 songs, going as far as to hit the ‚ÄúBeat It‚Äù steps with a sequined glove in the\\n          video. Here, Kendrick finally, formally casts himself as direct opposition, ending his verse\\n          with a haymaker referencing MJ‚Äôs own longtime Cold War enemy: ‚ÄúPrince outlived Mike\\n          Jack.‚Äù Sheesh.\\nAccording to the article, Drake released a song called \"The Heart Part 6\" in response to Kendrick Lamar\\'s diss track. This song is part of the ongoing beef between the two rappers.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nresponse = chat_engine.chat(\"What about Kendrick?\")\\nprint(str(response))\\n\\nresponse = chat_engine.chat(\"What about Kendrick?\")\\nprint(str(response))\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCondensed question: What did Kendrick Lamar release in response to Drake\\'s \"The Heart Part 6\"?\\nContext: https://www.gq.com/story/the-kendrick-lamar-drake-beef-explained                                        27/34\\n---\\n     5/10/24, 10:08 PM                                        The Kendrick Lamar/Drake Beef, Explained | GQ\\n          May 5: Drake hits back with ‚ÄúThe Heart Part 6‚Äù\\n                     The HEART PART 6   DRAKE\\n\\n\\nTHE HEART PART 6 - DRAKETHE HEART PART 6 - DRAKE\\n          The most productive weekend in the history of rap beef continues, with Drake saying\\n          fuck all to the Sunday Truce and doing exactly what Joe Budden advised: to hit back at\\n          Kendrick\\'s onslaught with a record in the vein of his time-stamp series, straight bars over\\n          a hard beat. Only, Kendrick already beat him to a time-stamp title last week with ‚Äú6:16\\n          in LA,‚Äù so Drake counters by co-opting one of Kendrick\\'s recurring series: ‚ÄúThe Heart.‚Äù\\n          (The last official entry, ‚ÄúThe Heart Part 5,‚Äù heralded Kendrick\\'s Mr. Morale and The Big\\n          Steppers album. Surely you remember the music video, where Kendrick applies deepfake\\n          technology to take on the visages of everyone from Kanye and Nipsey Hussle to OJ.)\\n          Drake even takes a page out of Kendrick\\'s diss manual and applies some classic soul to\\n          the proceedings, countering Kendrick\\'s Teddy Pendergrass and Al Green samples (on\\n          ‚ÄúEuphoria‚Äù and ‚Äú6:16,‚Äù respectively) with an Aretha Franklin sample here.\\n          Aretha sings ‚ÄúLet me see you proooove it,‚Äù setting the tone for Drake\\'s angle here that\\n          Kendrick\\'s been hitting him with baseless accusations. ‚ÄúThe Heart Part 6‚Äù is in full\\n          reaction mode to everything that\\'s transpired over the last three days, including direct\\n         Maial\\n          rebuttals to Kendrick\\'s ‚ÄúNot Like Us;‚Äù it was clearly written in the last 24 hours. DrakeSign up for Manual, our new flagship newsletter\\n                                   Useful advice on style, health, and more, four days a week.\\n          sounds‚Ä¶a little over it all, while nevertheless still promising that shit is about to get\\n          dark. (This is now his second track in a row where he plainly states he\\'d rather be on\\n\\n\\n\\n     https://www.gq.com/story/the-kendrick-lamar-drake-beef-explained                                                              28/34\\n---\\n5/10/24, 10:08 PM                                        The Kendrick Lamar/Drake Beef, Explained | GQ\\n     vacation somewhere than holed up in cold Toronto writing disses.) Drake, buddy,\\n     domestic abuse and pedophilia accusations are in the air‚Äîwe\\'ve been pitch black for the\\n     last few songs already.\\n\\n\\n\\n     You would think Drake would sound a little more celebratory than he does to start the\\n     song, where he takes a victory lap for allegedly going full Sydney Bristow and triple-\\n     crossing Kendrick into leaping on Fake Child Intel. ‚ÄúWe plotted for a week and then we\\n     fed you the information‚Ä¶we thought about giving a fake name or a destination/but you\\n     so thirsty, you not concerned with investigation.‚Äù Who\\'s lying or who was fooled? Only\\n     the Pusha T Investigative Team can solve this.\\n\\n\\n\\n     Drake doesn\\'t dwell there, though, instead moving on to Kendrick\\'s family, doubling\\n     down on the two angles that formed the basis of ‚ÄúFamily Matters‚Äù: that Kendrick has\\n     beaten his partner Whitney in the past, he\\'s estranged from their family, and one of his\\n     two kids is actually fathered by his friend and creative partner Dave Free. To drive this\\n     last point home, ‚ÄúThe Heart Part 6‚Äù artwork is an Instagram screenshot of Dave leaving\\n     heart emojis under, presumably, a picture Whitney posted.\\n\\n\\n\\n     Continuing his through line of using Kendrick\\'s confessional raps on Mr. Morale as\\n     ammo, Drake refers back to ‚ÄúMother I Sober,‚Äù the track where Kendrick unpacks his\\n     mother\\'s sexual abuse and how it informed an incident in his childhood where his\\n     mother was worried he was being abused by a family member even though Kendrick says\\n     he wasn\\'t. Dr. Drake\\'s read: He actually was molseted, and that\\'s why he\\'s so hell-bent\\n     on calling OVO ‚Äúcertified pedophiles.‚Äù\\n\\nCole would go on to respond to Kendrick with ‚Äú7 Minute Drill,‚Äù a diss track more\\n     notable for Cole admitting on it that he doesn\\'t want to really go there with his onetime\\n     friend than any especially vicious jabs. As the internet spent the weekend debating if J.\\n     Cole\\'s heart was really in it, by Sunday he would go onstage at his own Dreamville\\n     Festival to confirm just that. He publicly retracted his diss, apologized to and bigged up\\n     Lamar, and even vowed to stay out of it even if Kendrick should respond to ‚ÄúDrill.‚Äù\\n\\n\\n\\n     So why do Future and Metro Boomin suddenly have issues with Drake after doing\\n     dozens of collaborations with him?\\n\\n\\n\\n     We‚Äôve gone hundreds of words without returning to the duo who delivered this moment:\\n     Future, the fourth face on that 2010s Rap Mount Rushmore, and Metro Boomin, the\\n     superproducer he‚Äôs made some of his most potent music with. There‚Äôs a deeper layer to\\n     Kendrick choosing a Future and Metro album as the stage to finally go at Drake: Metro\\n     has seemingly had his own problems with the 6ix God. Late last year he posted and\\n     subsequently deleted a tweet about his acclaimed album Heroes and Villains continuing to\\n     lose awards to Drake (and frequent Metro collaborator 21 Savage‚Äôs) album Her Loss.\\n    Maial\\n     During a livestream not long after, Drake hilariously referenced ‚Äúthe non-believers, the\\n                              Sign up for Manual, our new flagship newsletter\\n                              Useful advice on style, health, and more, four days a week.\\n     underachievers, the tweet-and-deleters,‚Äù adding ‚Äúyou guys make me sick to my stomach,\\n\\n\\n\\nhttps://www.gq.com/story/the-kendrick-lamar-drake-beef-explained                                        6/34\\n---\\n5/10/24, 10:08 PM                                        The Kendrick Lamar/Drake Beef, Explained | GQ\\n     fam.‚Äù Despite trading a few more subliminal potshots across Twitter and IG, Metro\\n     downplayed any beef, saying that the issue was ‚Äúnot deep at all.‚Äù\\n     Still, when eagle-eyed fans took note of Metro unfollowing Drake on Instagram‚Äîthe\\n     definitive 21st century signpost of an un-amicable split‚Äîahead of the album‚Äôs release, it\\n     didn‚Äôt take a hip-hop scholar to assume that, as Kendrick would declare, ‚Äúit‚Äôs up.‚Äù And\\n     for those wondering how a producer-rapper beef would even reasonably play out, Metro\\n     makes it clear by serving up a new creative peak on ‚ÄúLike That,‚Äù with an obscenely\\n     screwface-inducing beat sampling 80s rap duo Rodney O and Joe Cooley\\'s classic\\n     ‚ÄúEverlasting Bass,‚Äù (which was famously earlier sampled on Three 6 Mafia‚Äôs ‚ÄúWho the\\n     Crunkest‚Äù,) alongside Eazy-E\\'s classic ‚ÄúEazy Duz It‚Äù as well as a splash of ‚ÄúRidin\\n     Spinners.‚Äù In effect Kendrick and Metro are following playbooks beloved by the likes of\\n     Jay-Z before them, or even Drake with ‚ÄúBack to Back,‚Äù in dissing your opponent on a\\n     song that‚Äôs an undeniable banger whether people know the context or not.\\n\\n\\n\\n     But why would Future, who has approximately 30 (thirty) collaborations with Drake,\\n     including the 2015 collab album What a Time to Be Alive and two fairly recent tracks on\\n     Future‚Äôs last solo album, cede airtime on his new project to a noted Drake enemy? No\\n     one knows for sure at press time, but it‚Äôs possible they have issues of their own. Despite\\n     their prolific collaborations, their relationship has had its rough moments from day one.\\n     Recall 2011, when an ascendant Future got an assist from Drake remixing the former‚Äôs\\n     ‚ÄúTony Montana,‚Äù only to publicly bemoan Drake refusing to do a video. And while they\\n     toured together in 2016, who can forget that time in 2013 when Future was briefly,\\n     allegedly booted off of Drake‚Äôs tour for less-than-flattering comments about his music in\\n     an interview.\\nAccording to the article, Kendrick Lamar released a song called \"6:16 in LA\" which was part of the beef.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n7. Agents¬∂Here we build agents with Llama 3. We perform RAG over simple functions as well as the documents above.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAgents And Tools¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nimport json\\nfrom typing import Sequence, List\\n\\nfrom llama_index.core.llms import ChatMessage\\nfrom llama_index.core.tools import BaseTool, FunctionTool\\nfrom llama_index.core.agent import ReActAgent\\n\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n\\nimport json\\nfrom typing import Sequence, List\\n\\nfrom llama_index.core.llms import ChatMessage\\nfrom llama_index.core.tools import BaseTool, FunctionTool\\nfrom llama_index.core.agent import ReActAgent\\n\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDefine Tools¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiple two integers and returns the result integer\"\"\"\\n    return a * b\\n\\n\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Add two integers and returns the result integer\"\"\"\\n    return a + b\\n\\n\\ndef subtract(a: int, b: int) -> int:\\n    \"\"\"Subtract two integers and returns the result integer\"\"\"\\n    return a - b\\n\\n\\ndef divide(a: int, b: int) -> int:\\n    \"\"\"Divides two integers and returns the result integer\"\"\"\\n    return a / b\\n\\n\\nmultiply_tool = FunctionTool.from_defaults(fn=multiply)\\nadd_tool = FunctionTool.from_defaults(fn=add)\\nsubtract_tool = FunctionTool.from_defaults(fn=subtract)\\ndivide_tool = FunctionTool.from_defaults(fn=divide)\\n\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiple two integers and returns the result integer\"\"\"\\n    return a * b\\n\\n\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Add two integers and returns the result integer\"\"\"\\n    return a + b\\n\\n\\ndef subtract(a: int, b: int) -> int:\\n    \"\"\"Subtract two integers and returns the result integer\"\"\"\\n    return a - b\\n\\n\\ndef divide(a: int, b: int) -> int:\\n    \"\"\"Divides two integers and returns the result integer\"\"\"\\n    return a / b\\n\\n\\nmultiply_tool = FunctionTool.from_defaults(fn=multiply)\\nadd_tool = FunctionTool.from_defaults(fn=add)\\nsubtract_tool = FunctionTool.from_defaults(fn=subtract)\\ndivide_tool = FunctionTool.from_defaults(fn=divide)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReAct Agent¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nagent = ReActAgent.from_tools(\\n    [multiply_tool, add_tool, subtract_tool, divide_tool],\\n    llm=llm_70b,\\n    verbose=True,\\n)\\n\\nagent = ReActAgent.from_tools(\\n    [multiply_tool, add_tool, subtract_tool, divide_tool],\\n    llm=llm_70b,\\n    verbose=True,\\n)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nQuerying¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nresponse = agent.chat(\"What is (121 + 2) * 5?\")\\nprint(str(response))\\n\\nresponse = agent.chat(\"What is (121 + 2) * 5?\")\\nprint(str(response))\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\\nAction: add\\nAction Input: {\\'a\\': 121, \\'b\\': 2}\\nObservation: 123\\nThought: Now I have the result of the addition, I need to multiply it by 5.\\nAction: multiply\\nAction Input: {\\'a\\': 123, \\'b\\': 5}\\nObservation: 615\\nThought: I can answer without using any more tools. I\\'ll use the user\\'s language to answer\\nAnswer: 615\\n615\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReAct Agent With RAG QueryEngine Tools¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_index.core import (\\n    SimpleDirectoryReader,\\n    VectorStoreIndex,\\n    StorageContext,\\n    load_index_from_storage,\\n)\\n\\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\\n\\nfrom llama_index.core import (\\n    SimpleDirectoryReader,\\n    VectorStoreIndex,\\n    StorageContext,\\n    load_index_from_storage,\\n)\\n\\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCreate ReAct Agent using RAG QueryEngine Tools¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\ndrake_tool = QueryEngineTool(\\n    drake_index.as_query_engine(),\\n    metadata=ToolMetadata(\\n        name=\"drake_search\",\\n        description=\"Useful for searching over Drake\\'s life.\",\\n    ),\\n)\\n\\nkendrick_tool = QueryEngineTool(\\n    kendrick_index.as_query_engine(),\\n    metadata=ToolMetadata(\\n        name=\"kendrick_search\",\\n        description=\"Useful for searching over Kendrick\\'s life.\",\\n    ),\\n)\\n\\nquery_engine_tools = [drake_tool, kendrick_tool]\\n\\ndrake_tool = QueryEngineTool(\\n    drake_index.as_query_engine(),\\n    metadata=ToolMetadata(\\n        name=\"drake_search\",\\n        description=\"Useful for searching over Drake\\'s life.\",\\n    ),\\n)\\n\\nkendrick_tool = QueryEngineTool(\\n    kendrick_index.as_query_engine(),\\n    metadata=ToolMetadata(\\n        name=\"kendrick_search\",\\n        description=\"Useful for searching over Kendrick\\'s life.\",\\n    ),\\n)\\n\\nquery_engine_tools = [drake_tool, kendrick_tool]\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nagent = ReActAgent.from_tools(\\n    query_engine_tools,  ## TODO: define query tools\\n    llm=llm_70b,\\n    verbose=True,\\n)\\n\\nagent = ReActAgent.from_tools(\\n    query_engine_tools,  ## TODO: define query tools\\n    llm=llm_70b,\\n    verbose=True,\\n)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nQuerying¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nresponse = agent.chat(\"Tell me about how Kendrick and Drake grew up\")\\nprint(str(response))\\n\\nresponse = agent.chat(\"Tell me about how Kendrick and Drake grew up\")\\nprint(str(response))\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\\nAction: kendrick_search\\nAction Input: {\\'input\\': \"Kendrick Lamar\\'s childhood\"}\\nObservation: Kendrick Lamar was born on June 17, 1987, in Compton, California. He is the first child of Kenneth \"Kenny\" Duckworth, a former gang hustler, and Paula Oliver, a hairdresser. Both of his parents are African Americans from the South Side of Chicago. When they were teenagers, they relocated to Compton in 1984 due to his father\\'s affiliation with the Gangster Disciples. Lamar was named after singer-songwriter Eddie Kendricks of the Temptations.\\nThought: I need more information about Drake\\'s childhood to compare their upbringings.\\nAction: drake_search\\nAction Input: {\\'input\\': \"Drake\\'s childhood\"}\\nObservation: Drake\\'s parents divorced when he was five years old. After the divorce, he and his mother remained in Toronto; his father returned to Memphis, where he was incarcerated for a number of years on drug-related charges.\\nThought: I have information about both Kendrick and Drake\\'s childhoods. I can now compare their upbringings.\\nAnswer: Kendrick Lamar grew up in Compton, California, with his parents, who were both from the South Side of Chicago. He was exposed to gang culture from a young age due to his father\\'s affiliation with the Gangster Disciples. On the other hand, Drake grew up in Toronto, Canada, with his mother after his parents\\' divorce when he was five years old. His father was incarcerated in Memphis for drug-related charges.\\nKendrick Lamar grew up in Compton, California, with his parents, who were both from the South Side of Chicago. He was exposed to gang culture from a young age due to his father\\'s affiliation with the Gangster Disciples. On the other hand, Drake grew up in Toronto, Canada, with his mother after his parents\\' divorce when he was five years old. His father was incarcerated in Memphis for drug-related charges.\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Llama3 Cookbook\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Llama3 Cookbook with Ollama and Replicate\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_groq/', 'title': 'Llama3 Cookbook with Groq - LlamaIndex', 'language': 'en'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOCAL PDF\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "PDF_PATH = \"/content/Financial-Guide.pdf\"\n",
        "\n",
        "loader = PyPDFLoader(PDF_PATH)\n",
        "pages = loader.load()\n",
        "#pages = loader.load_and_split()\n",
        "print(\"# of pages : \",len(pages))\n",
        "print(pages[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElSi5K5Q0iHC",
        "outputId": "2733a7ec-cb44-4406-d361-a755295090a8"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of pages :  44\n",
            "page_content='The Basics of Financial Management\\nfor Small-community Utilities\\nRCAPRURAL COMMUNITY ASSISTANCE PARTNERSHIP\\nan equal opportunity provider and employer\\n' metadata={'source': '/content/Financial-Guide.pdf', 'page': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA SPLIT\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Split to docs\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100, add_start_index=True)\n",
        "docs = splitter.split_documents(data)\n",
        "print('number of docs : ',len(docs))\n",
        "print(docs[0].metadata)\n",
        "#print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyBsNzz1hFBB",
        "outputId": "54289dc8-761e-41e6-bbf4-de0599e75d20"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of docs :  119\n",
            "{'source': 'https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_groq/', 'title': 'Llama3 Cookbook with Groq - LlamaIndex', 'language': 'en', 'start_index': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EMBEDDING\n",
        "from langchain_cohere import CohereEmbeddings\n",
        "\n",
        "embedding = CohereEmbeddings(model=\"embed-multilingual-v3.0\", cohere_api_key=COHERE_API)\n",
        "#embedding = CohereEmbeddings(model=\"embed-english-light-v3.0\", api_key=COHERE_API)\n",
        "embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PsCLd_IUl0n",
        "outputId": "eeee11b6-29a3-48fa-87d5-17a09fbc1aa2"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CohereEmbeddings(client=<cohere.client.Client object at 0x7bb67d4a0520>, async_client=<cohere.client.AsyncClient object at 0x7bb67d4a2e30>, model='embed-multilingual-v3.0', truncate=None, cohere_api_key='qiXeDEpa9LzgLxETqRvyHHCCNAAbyBcvUK2F0Eqh', max_retries=3, request_timeout=None, user_agent='langchain:partner', base_url=None)"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CHROMA VectorDB\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
        "vectorstore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFhQUacVzXYn",
        "outputId": "21e5f941-6389-4513-f336-98372a0c930d"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_chroma.vectorstores.Chroma at 0x7bb67d3f25c0>"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RETRIEVER\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
        "retrieved_docs = retriever.invoke(\"What is Groq ?\")\n",
        "\n",
        "print(len(retrieved_docs))\n",
        "print(retrieved_docs[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVXB9rdLl7mL",
        "outputId": "2a8d123b-8907-4cfe-b641-86f56b163b01"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "page_content='Llama3 Cookbook with Groq - LlamaIndex\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LlamaIndex\\n          \\n\\n\\n\\n            \\n              Llama3 Cookbook with Groq\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n    \\n  \\n  Home\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Learn\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Use Cases\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Component Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Advanced Topics\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  API Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Open-Source Community\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  LlamaCloud\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LlamaIndex\\n  \\n\\n\\n\\n\\n\\n\\n    Home\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Home\\n          \\n\\n\\n\\n\\n    High-Level Concepts (RAG)\\n  \\n\\n\\n\\n\\n\\n    Installation and Setup\\n  \\n\\n\\n\\n\\n\\n    How to read these docs\\n  \\n\\n\\n\\n\\n\\n\\n    Starter Examples' metadata={'language': 'en', 'source': 'https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_groq/', 'start_index': 10, 'title': 'Llama3 Cookbook with Groq - LlamaIndex'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"The following is a friendly conversation between a human and an asssistant.\"\n",
        "    \"The assistant is talkative and provides lots of specific details from its context\\n\"\n",
        "    \"Use the following pieces of retrieved context to answer \"\n",
        "    \"the question.\\n If you don't know the answer, say that you \"\n",
        "    \"don't know. Don't make up answers \\n\"\n",
        "    \"Keep the answer detailed, professional and concise.\"\n",
        "    \"\\nContext :\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "prompt.messages[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPXQQJ89ptqm",
        "outputId": "83dbe486-85e5-44e2-a170-f15f7c05548c"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an assistant for question-answering tasks. The following is a friendly conversation between a human and an asssistant.The assistant is talkative and provides lots of specific details from its context\\nUse the following pieces of retrieved context to answer the question.\\n If you don't know the answer, say that you don't know. Don't make up answers \\nKeep the answer detailed, professional and concise.\\nContext :\\n{context}\"))"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "\n",
        "QA_CHAIN = create_stuff_documents_chain(llm, prompt)\n",
        "RAG_CHAIN = create_retrieval_chain(retriever, QA_CHAIN)\n",
        "\n",
        "response = RAG_CHAIN.invoke({\"input\": \"How to use groq ?\"})\n",
        "print(\"ANSWER : \",response[\"answer\"])\n",
        "print(\"SOURCES : \",response[\"context\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tUnFm1Fnwuw",
        "outputId": "27104680-33f5-4368-9d4a-e0315c123e5a"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANSWER :  To use Groq, you need to follow these steps:\n",
            "1. Install the required packages:\n",
            "   ```python\n",
            "   !pip install llama-index\n",
            "   !pip install llama-index-llms-groq\n",
            "   ```\n",
            "\n",
            "2. Import the necessary modules and set up asynchronous processing:\n",
            "   ```python\n",
            "   import nest_asyncio\n",
            "   nest_asyncio.apply()\n",
            "   ```\n",
            "\n",
            "3. Set the GROQ_API_KEY environment variable:\n",
            "   ```python\n",
            "   import os\n",
            "   os.environ[\"GROQ_API_KEY\"] = \"<YOUR_GROQ_API_KEY>\"\n",
            "   ```\n",
            "\n",
            "4. Initialize the Groq LLM:\n",
            "   ```python\n",
            "   from llama_index.llms.groq import Groq\n",
            "   llm = Groq(model=\"llama3-8b-8192\")\n",
            "   ```\n",
            "\n",
            "Replace `<YOUR_GROQ_API_KEY>` with your actual Groq API key.\n",
            "\n",
            "With these steps, you can utilize the Groq LLM for various language modeling tasks, such as text generation, completion, and more. Make sure to refer to the Groq documentation and tutorials for further guidance on how to use their API and models effectively.\n",
            "SOURCES :  [Document(page_content='Llama3 Cookbook with Groq - LlamaIndex\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LlamaIndex\\n          \\n\\n\\n\\n            \\n              Llama3 Cookbook with Groq\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n    \\n  \\n  Home\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Learn\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Use Cases\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Component Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Advanced Topics\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  API Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Open-Source Community\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  LlamaCloud\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LlamaIndex\\n  \\n\\n\\n\\n\\n\\n\\n    Home\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Home\\n          \\n\\n\\n\\n\\n    High-Level Concepts (RAG)\\n  \\n\\n\\n\\n\\n\\n    Installation and Setup\\n  \\n\\n\\n\\n\\n\\n    How to read these docs\\n  \\n\\n\\n\\n\\n\\n\\n    Starter Examples\\n  \\n\\n\\n\\n\\n\\n            Starter Examples\\n          \\n\\n\\n\\n\\n    Starter Tutorial (OpenAI)\\n  \\n\\n\\n\\n\\n\\n    Starter Tutorial (Local Models)\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Discover LlamaIndex Video Series\\n  \\n\\n\\n\\n\\n\\n    Frequently Asked Questions (FAQ)\\n  \\n\\n\\n\\n\\n\\n\\n\\n    Starter Tools\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Starter Tools\\n          \\n\\n\\n\\n\\n    RAG CLI\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Learn\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Learn\\n          \\n\\n\\n\\n\\n    Using LLMs\\n  \\n\\n\\n\\n\\n\\n\\n    Loading & Ingestion\\n  \\n\\n\\n\\n\\n\\n            Loading & Ingestion\\n          \\n\\n\\n\\n\\n    Loading Data (Ingestion)\\n  \\n\\n\\n\\n\\n\\n    LlamaHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Indexing & Embedding\\n  \\n\\n\\n\\n\\n\\n    Storing\\n  \\n\\n\\n\\n\\n\\n    Querying\\n  \\n\\n\\n\\n\\n\\n    Tracing and Debugging\\n  \\n\\n\\n\\n\\n\\n\\n    Evaluating\\n  \\n\\n\\n\\n\\n\\n            Evaluating\\n          \\n\\n\\n\\n\\n    Evaluating\\n  \\n\\n\\n\\n\\n\\n\\n\\n    Cost Analysis\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Cost Analysis\\n          \\n\\n\\n\\n\\n    Usage Pattern\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Putting it all Together\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Putting it all Together\\n          \\n\\n\\n\\n\\n    Agents\\n  \\n\\n\\n\\n\\n\\n    Full-Stack Web Application', metadata={'language': 'en', 'source': 'https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_groq/', 'start_index': 10, 'title': 'Llama3 Cookbook with Groq - LlamaIndex'}), Document(page_content='Llama3 Cookbook with Groq - LlamaIndex\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LlamaIndex\\n          \\n\\n\\n\\n            \\n              Llama3 Cookbook with Groq\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n    \\n  \\n  Home\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Learn\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Use Cases\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Component Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Advanced Topics\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  API Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Open-Source Community\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  LlamaCloud\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LlamaIndex\\n  \\n\\n\\n\\n\\n\\n\\n    Home\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Home\\n          \\n\\n\\n\\n\\n    High-Level Concepts (RAG)\\n  \\n\\n\\n\\n\\n\\n    Installation and Setup\\n  \\n\\n\\n\\n\\n\\n    How to read these docs\\n  \\n\\n\\n\\n\\n\\n\\n    Starter Examples', metadata={'language': 'en', 'source': 'https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_groq/', 'start_index': 10, 'title': 'Llama3 Cookbook with Groq - LlamaIndex'}), Document(page_content='Cookbooks\\n  \\n\\n\\n\\n\\n\\n            Cookbooks\\n          \\n\\n\\n\\n\\n    Cohere init8 and binary Embeddings Retrieval Evaluation\\n  \\n\\n\\n\\n\\n\\n    mixedbread Rerank Cookbook\\n  \\n\\n\\n\\n\\n\\n    MistralAI Cookbook\\n  \\n\\n\\n\\n\\n\\n    Anthropic Haiku Cookbook\\n  \\n\\n\\n\\n\\n\\n    Llama3 Cookbook\\n  \\n\\n\\n\\n\\n\\n\\n    Llama3 Cookbook with Groq\\n  \\n\\n\\n\\n\\n    Llama3 Cookbook with Groq\\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Installation and Setup\\n    \\n\\n\\n\\n\\n\\n\\n      Setup LLM using Groq\\n    \\n\\n\\n\\n\\n\\n      Setup Embedding Model\\n    \\n\\n\\n\\n\\n\\n      Define Global Settings Configuration\\n    \\n\\n\\n\\n\\n\\n      Download Data\\n    \\n\\n\\n\\n\\n\\n      Load Data\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      1. Basic Completion and Chat\\n    \\n\\n\\n\\n\\n\\n\\n      Call complete with a prompt\\n    \\n\\n\\n\\n\\n\\n      Call chat with a list of messages\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      2. Basic RAG (Vector Search, Summarization)\\n    \\n\\n\\n\\n\\n\\n\\n      Basic RAG (Vector Search)\\n    \\n\\n\\n\\n\\n\\n      Basic RAG (Summarization)\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      3. Advanced RAG (Routing)', metadata={'language': 'en', 'source': 'https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_groq/', 'start_index': 5402, 'title': 'Llama3 Cookbook with Groq - LlamaIndex'}), Document(page_content='Table of contents\\n    \\n\\n\\n\\n\\n      Installation and Setup\\n    \\n\\n\\n\\n\\n\\n\\n      Setup LLM using Groq\\n    \\n\\n\\n\\n\\n\\n      Setup Embedding Model\\n    \\n\\n\\n\\n\\n\\n      Define Global Settings Configuration\\n    \\n\\n\\n\\n\\n\\n      Download Data\\n    \\n\\n\\n\\n\\n\\n      Load Data\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      1. Basic Completion and Chat\\n    \\n\\n\\n\\n\\n\\n\\n      Call complete with a prompt\\n    \\n\\n\\n\\n\\n\\n      Call chat with a list of messages\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      2. Basic RAG (Vector Search, Summarization)\\n    \\n\\n\\n\\n\\n\\n\\n      Basic RAG (Vector Search)\\n    \\n\\n\\n\\n\\n\\n      Basic RAG (Summarization)\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      3. Advanced RAG (Routing)\\n    \\n\\n\\n\\n\\n\\n\\n      Build a Router that can choose whether to do vector search or summarization\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      4. Text-to-SQL\\n    \\n\\n\\n\\n\\n\\n      5. Structured Data Extraction\\n    \\n\\n\\n\\n\\n\\n      6. Adding Chat History to RAG (Chat Engine)\\n    \\n\\n\\n\\n\\n\\n      7. Agents\\n    \\n\\n\\n\\n\\n\\n\\n      Agents And Tools\\n    \\n\\n\\n\\n\\n\\n      Define Tools\\n    \\n\\n\\n\\n\\n\\n      ReAct Agent\\n    \\n\\n\\n\\n\\n\\n      Querying\\n    \\n\\n\\n\\n\\n\\n      ReAct Agent With RAG QueryEngine Tools\\n    \\n\\n\\n\\n\\n\\n      Create ReAct Agent using RAG QueryEngine Tools\\n    \\n\\n\\n\\n\\n\\n      Querying\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Llama3 Cookbook with Ollama and Replicate\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Customization\\n  \\n\\n\\n\\n\\n\\n            Customization\\n          \\n\\n\\n\\n\\n    Streaming for Chat Engine - Condense Question Mode\\n  \\n\\n\\n\\n\\n\\n    Streaming\\n  \\n\\n\\n\\n\\n\\n    Completion Prompts Customization\\n  \\n\\n\\n\\n\\n\\n    Chat Prompts Customization\\n  \\n\\n\\n\\n\\n\\n    ChatGPT\\n  \\n\\n\\n\\n\\n\\n    HuggingFace LLM - StableLM\\n  \\n\\n\\n\\n\\n\\n    HuggingFace LLM - Camel-5b\\n  \\n\\n\\n\\n\\n\\n    Azure OpenAI\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Data Connectors\\n  \\n\\n\\n\\n\\n\\n            Data Connectors\\n          \\n\\n\\n\\n\\n    Parallel Processing SimpleDirectoryReader\\n  \\n\\n\\n\\n\\n\\n    DeepLake Reader\\n  \\n\\n\\n\\n\\n\\n    Psychic Reader\\n  \\n\\n\\n\\n\\n\\n    Qdrant Reader\\n  \\n\\n\\n\\n\\n\\n    HTML Tag Reader\\n  \\n\\n\\n\\n\\n\\n    Discord Reader\\n  \\n\\n\\n\\n\\n\\n    MongoDB Reader\\n  \\n\\n\\n\\n\\n\\n    Chroma Reader\\n  \\n\\n\\n\\n\\n\\n    MyScale Reader\\n  \\n\\n\\n\\n\\n\\n    Faiss Reader\\n  \\n\\n\\n\\n\\n\\n    Obsidian Reader\\n  \\n\\n\\n\\n\\n\\n    Slack Reader\\n  \\n\\n\\n\\n\\n\\n    Web Page Reader', metadata={'language': 'en', 'source': 'https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_groq/', 'start_index': 5741, 'title': 'Llama3 Cookbook with Groq - LlamaIndex'}), Document(page_content='Querying\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLlama3 Cookbook with Groq¬∂\\nMeta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks.\\nIn this notebook, we demonstrate how to use Llama3 with LlamaIndex for a comprehensive set of use cases.\\n\\nBasic completion / chat\\nBasic RAG (Vector Search, Summarization)\\nAdvanced RAG (Routing)\\nText-to-SQL\\nStructured Data Extraction\\nChat Engine + Memory\\nAgents\\n\\nWe use Llama3-8B and Llama3-70B through Groq.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInstallation and Setup¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\n!pip install llama-index\\n!pip install llama-index-llms-groq\\n!pip install llama-index-embeddings-huggingface\\n!pip install llama-parse\\n\\n!pip install llama-index\\n!pip install llama-index-llms-groq\\n!pip install llama-index-embeddings-huggingface\\n!pip install llama-parse\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSetup LLM using Groq¬∂To use Groq, you need to make sure that GROQ_API_KEY is specified as an environment variable.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nimport os\\n\\nos.environ[\"GROQ_API_KEY\"] = \"<GROQ_API_KEY>\"\\n\\nimport os\\n\\nos.environ[\"GROQ_API_KEY\"] = \"\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_index.llms.groq import Groq\\n\\nllm = Groq(model=\"llama3-8b-8192\")\\nllm_70b = Groq(model=\"llama3-70b-8192\")\\n\\nfrom llama_index.llms.groq import Groq\\n\\nllm = Groq(model=\"llama3-8b-8192\")\\nllm_70b = Groq(model=\"llama3-70b-8192\")\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSetup Embedding Model¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\\n\\nembed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")', metadata={'language': 'en', 'source': 'https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_groq/', 'start_index': 49586, 'title': 'Llama3 Cookbook with Groq - LlamaIndex'}), Document(page_content='Installation and Setup¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\n!pip install llama-index\\n!pip install llama-index-llms-groq\\n!pip install llama-index-embeddings-huggingface\\n!pip install llama-parse\\n\\n!pip install llama-index\\n!pip install llama-index-llms-groq\\n!pip install llama-index-embeddings-huggingface\\n!pip install llama-parse\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSetup LLM using Groq¬∂To use Groq, you need to make sure that GROQ_API_KEY is specified as an environment variable.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nimport os\\n\\nos.environ[\"GROQ_API_KEY\"] = \"<GROQ_API_KEY>\"\\n\\nimport os\\n\\nos.environ[\"GROQ_API_KEY\"] = \"\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_index.llms.groq import Groq\\n\\nllm = Groq(model=\"llama3-8b-8192\")\\nllm_70b = Groq(model=\"llama3-70b-8192\")\\n\\nfrom llama_index.llms.groq import Groq', metadata={'language': 'en', 'source': 'https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_groq/', 'start_index': 50320, 'title': 'Llama3 Cookbook with Groq - LlamaIndex'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAA\n",
        "---"
      ],
      "metadata": {
        "id": "MFZl9z-xa7mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import streamlit as st\n",
        "#from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "dj84Z4a4nxPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a sample vectorDB\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "# RAG\n",
        "from langchain_cohere import CohereEmbeddings\n",
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "kXTG6dFwn08J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlXylErVzpLG",
        "outputId": "3b502834-c650-44b2-f95a-9fd09a3bd000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Jump to ContentGuides and ConceptsAPI ReferenceRelease NotesLLMUCoralDashboardDocumentationPlaygroundCommunityLog InCoralDashboardDocumentationPlaygroundCommunityLog InGuides and ConceptsAPI ReferenceRelease NotesLLMULoading‚Ä¶SearchGet StartedThe Cohere PlatformIntroduction to Large Language ModelsDeveloper PlaygroundCohere ToolkitModelsCommand R+Command RCommandEmbedRerankText generationUsing the Chat APIStreaming ResponsesPredictable OutputsRetrieval Augmented Generation (RAG)Tool UseSingle-Step Tool Use (Function Calling)Multi-step Tool Use (Agents)Parameter Types in Tool UseTokens and TokenizersPrompt EngineeringCrafting Effective PromptsAdvanced Prompt Engineering TechniquesPreamblesPrompt LibraryCreate CSV data from JSON dataCreate a markdown table from raw dataMeeting SummarizerRemove PIIAdd a Docstring to your codeEvaluate your LLM responseFaster Web SearchMultilingual interpreterRAG ConnectorsOverviewCreating and Deploying a ConnectorManaging your ConnectorConnector AuthenticationConnector FAQsMigrating from the Generate API to the Chat APIText embeddings (vectors, search, retrieval)EmbeddingsUsing the Embed APIBatch Embedding JobsRerankingOverviewBest PracticesMultilingual Embed ModelsMultilingual Semantic SearchCustomer Feedback AggregationCross-Lingual Content ModerationUsing the Dataset APIFine-tuningIntroductionFine-tuning with the Cohere dashboardProgrammatic Fine-tuningFine-tuning for ChatPreparing the Chat Fine-tuning DataStarting the Chat Fine-TuningUnderstanding the Chat Fine-tuning ResultsImproving the Chat Fine-tuning ResultsFine-tuning for ClassifyPreparing the Classify Fine-tuning dataStarting the Classify Fine-TuningUnderstanding the Classify Fine-tuning ResultsImproving the Classify Fine-tuning ResultsFine-tuning for RerankPreparing the Rerank Fine-tuning DataStarting the Rerank Fine-TuningUnderstanding the Rerank Fine-tuning ResultsImproving the Rerank Fine-tuning ResultsFAQs / TroubleshootingGoing to productionAPI Keys and Rate LimitsGoing LiveInTegrationsEmbedding IntegrationsElasticsearch and CohereMongoDB and CohereRedis and CohereHaystack and CohereOpen Search and CohereVespa and CohereChroma and CohereQdrant and CohereWeaviate and CoherePinecone and CohereMilvus and CohereLangChainLlamaIndexDeployment OptionsCohere on AWSAmazon BedrockAmazon SageMakerCohere on AzureSingle Container on Private CloudstutorialsSemantic SearchContent ModerationEntity ExtractionText ClassificationText Classification (Classify)Text Classification (Embed)Customer SupportIntent RecognitionSentiment AnalysisToxicity DetectionResponsible UseOverviewUsage GuidelinesModel LimitationsData StatementGeneration BenchmarksRepresentation BenchmarksSecurityEnvironmental ImpactLLM UniversityWelcome to LLM University!Structure of the CourseYour InstructorsDiscord and CommunityBrief intro: What is NLP and LLMs?Module 1: What are Large Language Models?Text EmbeddingsSimilarity Between Words and SentencesThe Attention MechanismTransformer ModelsConclusion - Large Language ModelsModule 2: Text RepresentationIntroduction to Text EmbeddingsIntroduction to Semantic SearchClustering With EmbeddingsTopic ModelingText ClassificationFew-Shot ClassificationFine-Tuning for ClassificationMultilingual Sentiment AnalysisConclusion - Text RepresentationModule 3: Text GenerationIntroduction to Text GenerationBuilding a ChatbotParameters for Controlling OutputsPrompt Engineering BasicsFine-Tuning for ChatIntroduction to RAGConclusion - Text GenerationModule 4: DeploymentDeploying with StreamlitDeploying with FastAPIDeploying on Google Sheets with Google Apps ScriptDeploying as a Chrome ExtensionDeploying with DatabuttonConclusionModule 5: Semantic SearchWhat is Semantic Search?Keyword SearchDense RetrievalReRankingGenerating AnswersFurther ReadingConclusion - Semantic SearchModule 6: Prompt EngineeringConstructing PromptsUse Case PatternsChaining PromptsValidating OutputsEvaluating OutputsConclusion - Prompt EngineeringModule 7: The Cohere PlatformServing PlatformFoundational ModelsEndpointsApplicationsConclusion - The Cohere PlatformModule 8: Retrieval-Augmented Generation (RAG)Appendix: NLP and ML FundamentalsHistory of NLPApplications of NLPText Pre-Processing in NLPHow to Convert Text Into VectorsPast Machine-Learning Methods of NLPHow to Build a ClassifierHow to Evaluate a ClassifierConclusion - NLPcohere for aiCohere For AI Acceptable Use PolicyRetrieval Augmented Generation (RAG)Suggest EditsRetrieval Augmented Generation (RAG) is a method for generating text using additional information fetched from an external data source. Providing relevant documents to the model can greatly increase the accuracy of the response. The Chat API in combination with a Command, Command R, or Command R+ makes it easy to generate text that is grounded on supplementary information.\\nFor example, the code snippet below will produce an answer to \"Where do the tallest penguins live?\" along with inline citations based on the provided documents.' metadata={'source': 'https://docs.cohere.com/docs/retrieval-augmented-generation-rag/', 'title': 'Retrieval Augmented Generation (RAG)', 'description': 'Retrieval Augmented Generation (RAG) is a method for generating text using external data sources to improve accuracy. The Chat API in combination with the Command model can help generate grounded text with inline citations based on provided documents.', 'language': 'en'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding\n",
        "COHERE_API = userdata.get('COHERE_API')\n",
        "embedding = CohereEmbeddings(model=\"embed-multilingual-v3.0\", cohere_api_key=COHERE_API)\n",
        "#embedding = CohereEmbeddings(model=\"embed-english-light-v3.0\", api_key=COHERE_API)\n",
        "# VectorDB\n",
        "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)"
      ],
      "metadata": {
        "id": "qS83UVQY0H_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MEMORY = ConversationBufferWindowMemory(k=5)"
      ],
      "metadata": {
        "id": "ga6v3Pyg9z_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  TEMPLATE = \"\"\"\n",
        "  The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context, and can go low level details. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
        "  \\nRelevant pieces of previous conversation:\n",
        "  {history}\n",
        "  \\n(You do not need to use these pieces of information if not relevant)\n",
        "  Current conversation:\n",
        "  Human: {input}\n",
        "  AI:\"\"\"\n",
        "  #"
      ],
      "metadata": {
        "id": "gc8odkxs8vtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "MODEL_NAME = \"llama3-8b-8192\"\n",
        "LLM = ChatGroq(model=MODEL_NAME, api_key=GROQ_API_KEY)\n",
        "\n",
        "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=TEMPLATE)"
      ],
      "metadata": {
        "id": "sq21jHCo9Oh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what this document is about?\"\n",
        "docs = vectordb.similarity_search(query)\n",
        "print(docs[3].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQKFJgL4jl2S",
        "outputId": "1e7d074e-8bd1-4444-9c9f-dc8fd4ad34c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\n",
            "In both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\n",
            "Reflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\n",
            "\n",
            "Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
            "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
            "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\n",
            "\n",
            "Fig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\n",
            "Chain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$, where $\\leq i \\leq j \\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\n",
            "To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\n",
            "The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\n",
            "\n",
            "Fig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\n",
            "The idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ZBOgWSW_a3j5"
      }
    }
  ]
}