{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKlNTPhAU7QOggtByDq67M"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install\n",
        "---"
      ],
      "metadata": {
        "id": "4MfCc_mue7pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --quiet langchain\n",
        "#!pip install --quiet -U langchain-cohere\n",
        "#!pip install --quiet langchain-community\n",
        "#!pip install --quiet langchain-chroma\n",
        "#!pip install unstructured\n",
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-wgqk7DhU7BK",
        "outputId": "98057759-3076-4ef3-e5f9-8207dd589bd4"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA & RAG CHAIN\n",
        "---"
      ],
      "metadata": {
        "id": "SIAIKPNc89t-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# keys\n",
        "from google.colab import userdata\n",
        "COHERE_API = userdata.get('COHERE_API')"
      ],
      "metadata": {
        "id": "gUaSZ8v_X58c"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ChatCOHERE LLM\n",
        "from langchain_cohere import ChatCohere\n",
        "\n",
        "llm = ChatCohere(model=\"command-r-plus\", cohere_api_key=COHERE_API)\n",
        "llm.invoke(\"hi\")"
      ],
      "metadata": {
        "id": "9UsS1fc_Ukqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53d3067e-4738-4432-b1da-2b080cc95576"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I help you today?', additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'f6950442-204f-4f69-8d03-a311396ceb1f', 'token_count': {'input_tokens': 67, 'output_tokens': 9}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'f6950442-204f-4f69-8d03-a311396ceb1f', 'token_count': {'input_tokens': 67, 'output_tokens': 9}}, id='run-d950be36-8b7a-4b48-815c-1a8872a3d679-0')"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WEBPAGE DATA LOAD\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "#URL = \"https://docs.cohere.com/docs/retrieval-augmented-generation-rag/\"\n",
        "#URL = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
        "URL = \"https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_groq/\"\n",
        "\n",
        "# Load blog post\n",
        "loader = WebBaseLoader(URL)\n",
        "data = loader.load()\n",
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qFSWedh7gBeM",
        "outputId": "be4f0f3f-bb39-4396-ed21-f3028daade3a"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOCAL PDF\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "PDF_PATH = \"/content/Financial-Guide.pdf\"\n",
        "\n",
        "loader = PyPDFLoader(PDF_PATH)\n",
        "pages = loader.load()\n",
        "#pages = loader.load_and_split()\n",
        "print(\"# of pages : \",len(pages))\n",
        "print(pages[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElSi5K5Q0iHC",
        "outputId": "2733a7ec-cb44-4406-d361-a755295090a8"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of pages :  44\n",
            "page_content='The Basics of Financial Management\\nfor Small-community Utilities\\nRCAPRURAL COMMUNITY ASSISTANCE PARTNERSHIP\\nan equal opportunity provider and employer\\n' metadata={'source': '/content/Financial-Guide.pdf', 'page': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA SPLIT\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Split to docs\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100, add_start_index=True)\n",
        "docs = splitter.split_documents(data)\n",
        "print('number of docs : ',len(docs))\n",
        "print(docs[0].metadata)\n",
        "#print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyBsNzz1hFBB",
        "outputId": "54289dc8-761e-41e6-bbf4-de0599e75d20"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of docs :  119\n",
            "{'source': 'https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_groq/', 'title': 'Llama3 Cookbook with Groq - LlamaIndex', 'language': 'en', 'start_index': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EMBEDDING\n",
        "from langchain_cohere import CohereEmbeddings\n",
        "\n",
        "embedding = CohereEmbeddings(model=\"embed-multilingual-v3.0\", cohere_api_key=COHERE_API)\n",
        "#embedding = CohereEmbeddings(model=\"embed-english-light-v3.0\", api_key=COHERE_API)\n",
        "embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PsCLd_IUl0n",
        "outputId": "eeee11b6-29a3-48fa-87d5-17a09fbc1aa2"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CohereEmbeddings(client=<cohere.client.Client object at 0x7bb67d4a0520>, async_client=<cohere.client.AsyncClient object at 0x7bb67d4a2e30>, model='embed-multilingual-v3.0', truncate=None, cohere_api_key='qiXeDEpa9LzgLxETqRvyHHCCNAAbyBcvUK2F0Eqh', max_retries=3, request_timeout=None, user_agent='langchain:partner', base_url=None)"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CHROMA VectorDB\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
        "vectorstore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFhQUacVzXYn",
        "outputId": "21e5f941-6389-4513-f336-98372a0c930d"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_chroma.vectorstores.Chroma at 0x7bb67d3f25c0>"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RETRIEVER\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
        "retrieved_docs = retriever.invoke(\"What is Groq ?\")\n",
        "\n",
        "print(len(retrieved_docs))\n",
        "print(retrieved_docs[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVXB9rdLl7mL",
        "outputId": "2a8d123b-8907-4cfe-b641-86f56b163b01"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "page_content='Llama3 Cookbook with Groq - LlamaIndex\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LlamaIndex\\n          \\n\\n\\n\\n            \\n              Llama3 Cookbook with Groq\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n    \\n  \\n  Home\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Learn\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Use Cases\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Component Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Advanced Topics\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  API Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Open-Source Community\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  LlamaCloud\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LlamaIndex\\n  \\n\\n\\n\\n\\n\\n\\n    Home\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Home\\n          \\n\\n\\n\\n\\n    High-Level Concepts (RAG)\\n  \\n\\n\\n\\n\\n\\n    Installation and Setup\\n  \\n\\n\\n\\n\\n\\n    How to read these docs\\n  \\n\\n\\n\\n\\n\\n\\n    Starter Examples' metadata={'language': 'en', 'source': 'https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_groq/', 'start_index': 10, 'title': 'Llama3 Cookbook with Groq - LlamaIndex'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"The following is a friendly conversation between a human and an asssistant.\"\n",
        "    \"The assistant is talkative and provides lots of specific details from its context\\n\"\n",
        "    \"Use the following pieces of retrieved context to answer \"\n",
        "    \"the question.\\n If you don't know the answer, say that you \"\n",
        "    \"don't know. Don't make up answers \\n\"\n",
        "    \"Keep the answer detailed, professional and concise.\"\n",
        "    \"\\nContext :\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "prompt.messages[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPXQQJ89ptqm",
        "outputId": "83dbe486-85e5-44e2-a170-f15f7c05548c"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an assistant for question-answering tasks. The following is a friendly conversation between a human and an asssistant.The assistant is talkative and provides lots of specific details from its context\\nUse the following pieces of retrieved context to answer the question.\\n If you don't know the answer, say that you don't know. Don't make up answers \\nKeep the answer detailed, professional and concise.\\nContext :\\n{context}\"))"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "\n",
        "QA_CHAIN = create_stuff_documents_chain(llm, prompt)\n",
        "RAG_CHAIN = create_retrieval_chain(retriever, QA_CHAIN)\n",
        "\n",
        "response = RAG_CHAIN.invoke({\"input\": \"How to use groq ?\"})\n",
        "print(\"ANSWER : \",response[\"answer\"])\n",
        "print(\"SOURCES : \",response[\"context\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tUnFm1Fnwuw",
        "outputId": "27104680-33f5-4368-9d4a-e0315c123e5a"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANSWER :  To use Groq, you need to follow these steps:\n",
            "1. Install the required packages:\n",
            "   ```python\n",
            "   !pip install llama-index\n",
            "   !pip install llama-index-llms-groq\n",
            "   ```\n",
            "\n",
            "2. Import the necessary modules and set up asynchronous processing:\n",
            "   ```python\n",
            "   import nest_asyncio\n",
            "   nest_asyncio.apply()\n",
            "   ```\n",
            "\n",
            "3. Set the GROQ_API_KEY environment variable:\n",
            "   ```python\n",
            "   import os\n",
            "   os.environ[\"GROQ_API_KEY\"] = \"<YOUR_GROQ_API_KEY>\"\n",
            "   ```\n",
            "\n",
            "4. Initialize the Groq LLM:\n",
            "   ```python\n",
            "   from llama_index.llms.groq import Groq\n",
            "   llm = Groq(model=\"llama3-8b-8192\")\n",
            "   ```\n",
            "\n",
            "Replace `<YOUR_GROQ_API_KEY>` with your actual Groq API key.\n",
            "\n",
            "With these steps, you can utilize the Groq LLM for various language modeling tasks, such as text generation, completion, and more. Make sure to refer to the Groq documentation and tutorials for further guidance on how to use their API and models effectively.\n",
            "SOURCES :  [Document(page_content='Llama3 Cookbook with Groq - LlamaIndex\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LlamaIndex\\n          \\n\\n\\n\\n            \\n              Llama3 Cookbook with Groq\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n    \\n  \\n  Home\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Learn\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Use Cases\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Component Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Advanced Topics\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  API Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Open-Source Community\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  LlamaCloud\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LlamaIndex\\n  \\n\\n\\n\\n\\n\\n\\n    Home\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Home\\n          \\n\\n\\n\\n\\n    High-Level Concepts (RAG)\\n  \\n\\n\\n\\n\\n\\n    Installation and Setup\\n  \\n\\n\\n\\n\\n\\n    How to read these docs\\n  \\n\\n\\n\\n\\n\\n\\n    Starter Examples\\n  \\n\\n\\n\\n\\n\\n            Starter Examples\\n          \\n\\n\\n\\n\\n    Starter Tutorial (OpenAI)\\n  \\n\\n\\n\\n\\n\\n    Starter Tutorial (Local Models)\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Discover LlamaIndex Video Series\\n  \\n\\n\\n\\n\\n\\n    Frequently Asked Questions (FAQ)\\n  \\n\\n\\n\\n\\n\\n\\n\\n    Starter Tools\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Starter Tools\\n          \\n\\n\\n\\n\\n    RAG CLI\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Learn\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Learn\\n          \\n\\n\\n\\n\\n    Using LLMs\\n  \\n\\n\\n\\n\\n\\n\\n    Loading & Ingestion\\n  \\n\\n\\n\\n\\n\\n            Loading & Ingestion\\n          \\n\\n\\n\\n\\n    Loading Data (Ingestion)\\n  \\n\\n\\n\\n\\n\\n    LlamaHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Indexing & Embedding\\n  \\n\\n\\n\\n\\n\\n    Storing\\n  \\n\\n\\n\\n\\n\\n    Querying\\n  \\n\\n\\n\\n\\n\\n    Tracing and Debugging\\n  \\n\\n\\n\\n\\n\\n\\n    Evaluating\\n  \\n\\n\\n\\n\\n\\n            Evaluating\\n          \\n\\n\\n\\n\\n    Evaluating\\n  \\n\\n\\n\\n\\n\\n\\n\\n    Cost Analysis\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Cost Analysis\\n          \\n\\n\\n\\n\\n    Usage Pattern\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Putting it all Together\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Putting it all Together\\n          \\n\\n\\n\\n\\n    Agents\\n  \\n\\n\\n\\n\\n\\n    Full-Stack Web Application', metadata={'language': 'en', 'source': 'https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_groq/', 'start_index': 10, 'title': 'Llama3 Cookbook with Groq - LlamaIndex'}), Document(page_content='Llama3 Cookbook with Groq - LlamaIndex\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LlamaIndex\\n          \\n\\n\\n\\n            \\n              Llama3 Cookbook with Groq\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n    \\n  \\n  Home\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Learn\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Use Cases\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Component Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Advanced Topics\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  API Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Open-Source Community\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  LlamaCloud\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LlamaIndex\\n  \\n\\n\\n\\n\\n\\n\\n    Home\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Home\\n          \\n\\n\\n\\n\\n    High-Level Concepts (RAG)\\n  \\n\\n\\n\\n\\n\\n    Installation and Setup\\n  \\n\\n\\n\\n\\n\\n    How to read these docs\\n  \\n\\n\\n\\n\\n\\n\\n    Starter Examples', metadata={'language': 'en', 'source': 'https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_groq/', 'start_index': 10, 'title': 'Llama3 Cookbook with Groq - LlamaIndex'}), Document(page_content='Cookbooks\\n  \\n\\n\\n\\n\\n\\n            Cookbooks\\n          \\n\\n\\n\\n\\n    Cohere init8 and binary Embeddings Retrieval Evaluation\\n  \\n\\n\\n\\n\\n\\n    mixedbread Rerank Cookbook\\n  \\n\\n\\n\\n\\n\\n    MistralAI Cookbook\\n  \\n\\n\\n\\n\\n\\n    Anthropic Haiku Cookbook\\n  \\n\\n\\n\\n\\n\\n    Llama3 Cookbook\\n  \\n\\n\\n\\n\\n\\n\\n    Llama3 Cookbook with Groq\\n  \\n\\n\\n\\n\\n    Llama3 Cookbook with Groq\\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Installation and Setup\\n    \\n\\n\\n\\n\\n\\n\\n      Setup LLM using Groq\\n    \\n\\n\\n\\n\\n\\n      Setup Embedding Model\\n    \\n\\n\\n\\n\\n\\n      Define Global Settings Configuration\\n    \\n\\n\\n\\n\\n\\n      Download Data\\n    \\n\\n\\n\\n\\n\\n      Load Data\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      1. Basic Completion and Chat\\n    \\n\\n\\n\\n\\n\\n\\n      Call complete with a prompt\\n    \\n\\n\\n\\n\\n\\n      Call chat with a list of messages\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      2. Basic RAG (Vector Search, Summarization)\\n    \\n\\n\\n\\n\\n\\n\\n      Basic RAG (Vector Search)\\n    \\n\\n\\n\\n\\n\\n      Basic RAG (Summarization)\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      3. Advanced RAG (Routing)', metadata={'language': 'en', 'source': 'https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_groq/', 'start_index': 5402, 'title': 'Llama3 Cookbook with Groq - LlamaIndex'}), Document(page_content='Table of contents\\n    \\n\\n\\n\\n\\n      Installation and Setup\\n    \\n\\n\\n\\n\\n\\n\\n      Setup LLM using Groq\\n    \\n\\n\\n\\n\\n\\n      Setup Embedding Model\\n    \\n\\n\\n\\n\\n\\n      Define Global Settings Configuration\\n    \\n\\n\\n\\n\\n\\n      Download Data\\n    \\n\\n\\n\\n\\n\\n      Load Data\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      1. Basic Completion and Chat\\n    \\n\\n\\n\\n\\n\\n\\n      Call complete with a prompt\\n    \\n\\n\\n\\n\\n\\n      Call chat with a list of messages\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      2. Basic RAG (Vector Search, Summarization)\\n    \\n\\n\\n\\n\\n\\n\\n      Basic RAG (Vector Search)\\n    \\n\\n\\n\\n\\n\\n      Basic RAG (Summarization)\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      3. Advanced RAG (Routing)\\n    \\n\\n\\n\\n\\n\\n\\n      Build a Router that can choose whether to do vector search or summarization\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      4. Text-to-SQL\\n    \\n\\n\\n\\n\\n\\n      5. Structured Data Extraction\\n    \\n\\n\\n\\n\\n\\n      6. Adding Chat History to RAG (Chat Engine)\\n    \\n\\n\\n\\n\\n\\n      7. Agents\\n    \\n\\n\\n\\n\\n\\n\\n      Agents And Tools\\n    \\n\\n\\n\\n\\n\\n      Define Tools\\n    \\n\\n\\n\\n\\n\\n      ReAct Agent\\n    \\n\\n\\n\\n\\n\\n      Querying\\n    \\n\\n\\n\\n\\n\\n      ReAct Agent With RAG QueryEngine Tools\\n    \\n\\n\\n\\n\\n\\n      Create ReAct Agent using RAG QueryEngine Tools\\n    \\n\\n\\n\\n\\n\\n      Querying\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Llama3 Cookbook with Ollama and Replicate\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Customization\\n  \\n\\n\\n\\n\\n\\n            Customization\\n          \\n\\n\\n\\n\\n    Streaming for Chat Engine - Condense Question Mode\\n  \\n\\n\\n\\n\\n\\n    Streaming\\n  \\n\\n\\n\\n\\n\\n    Completion Prompts Customization\\n  \\n\\n\\n\\n\\n\\n    Chat Prompts Customization\\n  \\n\\n\\n\\n\\n\\n    ChatGPT\\n  \\n\\n\\n\\n\\n\\n    HuggingFace LLM - StableLM\\n  \\n\\n\\n\\n\\n\\n    HuggingFace LLM - Camel-5b\\n  \\n\\n\\n\\n\\n\\n    Azure OpenAI\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Data Connectors\\n  \\n\\n\\n\\n\\n\\n            Data Connectors\\n          \\n\\n\\n\\n\\n    Parallel Processing SimpleDirectoryReader\\n  \\n\\n\\n\\n\\n\\n    DeepLake Reader\\n  \\n\\n\\n\\n\\n\\n    Psychic Reader\\n  \\n\\n\\n\\n\\n\\n    Qdrant Reader\\n  \\n\\n\\n\\n\\n\\n    HTML Tag Reader\\n  \\n\\n\\n\\n\\n\\n    Discord Reader\\n  \\n\\n\\n\\n\\n\\n    MongoDB Reader\\n  \\n\\n\\n\\n\\n\\n    Chroma Reader\\n  \\n\\n\\n\\n\\n\\n    MyScale Reader\\n  \\n\\n\\n\\n\\n\\n    Faiss Reader\\n  \\n\\n\\n\\n\\n\\n    Obsidian Reader\\n  \\n\\n\\n\\n\\n\\n    Slack Reader\\n  \\n\\n\\n\\n\\n\\n    Web Page Reader', metadata={'language': 'en', 'source': 'https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_groq/', 'start_index': 5741, 'title': 'Llama3 Cookbook with Groq - LlamaIndex'}), Document(page_content='Querying\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLlama3 Cookbook with Groq¬∂\\nMeta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks.\\nIn this notebook, we demonstrate how to use Llama3 with LlamaIndex for a comprehensive set of use cases.\\n\\nBasic completion / chat\\nBasic RAG (Vector Search, Summarization)\\nAdvanced RAG (Routing)\\nText-to-SQL\\nStructured Data Extraction\\nChat Engine + Memory\\nAgents\\n\\nWe use Llama3-8B and Llama3-70B through Groq.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInstallation and Setup¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\n!pip install llama-index\\n!pip install llama-index-llms-groq\\n!pip install llama-index-embeddings-huggingface\\n!pip install llama-parse\\n\\n!pip install llama-index\\n!pip install llama-index-llms-groq\\n!pip install llama-index-embeddings-huggingface\\n!pip install llama-parse\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSetup LLM using Groq¬∂To use Groq, you need to make sure that GROQ_API_KEY is specified as an environment variable.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nimport os\\n\\nos.environ[\"GROQ_API_KEY\"] = \"<GROQ_API_KEY>\"\\n\\nimport os\\n\\nos.environ[\"GROQ_API_KEY\"] = \"\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_index.llms.groq import Groq\\n\\nllm = Groq(model=\"llama3-8b-8192\")\\nllm_70b = Groq(model=\"llama3-70b-8192\")\\n\\nfrom llama_index.llms.groq import Groq\\n\\nllm = Groq(model=\"llama3-8b-8192\")\\nllm_70b = Groq(model=\"llama3-70b-8192\")\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSetup Embedding Model¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\\n\\nembed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")', metadata={'language': 'en', 'source': 'https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_groq/', 'start_index': 49586, 'title': 'Llama3 Cookbook with Groq - LlamaIndex'}), Document(page_content='Installation and Setup¬∂\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\n!pip install llama-index\\n!pip install llama-index-llms-groq\\n!pip install llama-index-embeddings-huggingface\\n!pip install llama-parse\\n\\n!pip install llama-index\\n!pip install llama-index-llms-groq\\n!pip install llama-index-embeddings-huggingface\\n!pip install llama-parse\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSetup LLM using Groq¬∂To use Groq, you need to make sure that GROQ_API_KEY is specified as an environment variable.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nimport os\\n\\nos.environ[\"GROQ_API_KEY\"] = \"<GROQ_API_KEY>\"\\n\\nimport os\\n\\nos.environ[\"GROQ_API_KEY\"] = \"\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn¬†[¬†]:\\n\\n\\n\\n\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\nfrom llama_index.llms.groq import Groq\\n\\nllm = Groq(model=\"llama3-8b-8192\")\\nllm_70b = Groq(model=\"llama3-70b-8192\")\\n\\nfrom llama_index.llms.groq import Groq', metadata={'language': 'en', 'source': 'https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook_groq/', 'start_index': 50320, 'title': 'Llama3 Cookbook with Groq - LlamaIndex'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ZBOgWSW_a3j5"
      }
    }
  ]
}